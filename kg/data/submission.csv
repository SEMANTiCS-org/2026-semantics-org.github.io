ID,track_id,track name,title,authors,submitted,last updated,form fields,keywords,decision,notified,reviews sent,abstract,deleted?,url,doi,img
2,2,sem24-research,Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification,"Delfina Sol Martinez Pandiani, Nicolas Lazzari and Valentina Presutti",2024-02-22 16:22,2024-02-29 15:48,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/Stitching-Gaps-B339/
(Data Availability) https://anonymous.4open.science/r/Stitching-Gaps-B339/
(Ontology Availability) 
(ORKG) ","Abstract Concepts
Neuro-Symbolic AI
Interpretability
Situated Knowledge
Knowledge Graph Embeddings
Vision Transformers
Image Classification",accept,yes,yes,"Background: The increasing demand for advanced image understanding, particularly in detecting abstract concepts (AC) in images, presents a multifaceted challenge both technically and ethically for humans and machines alike. This demand highlights the necessity for innovative and more interpretable approaches that reconcile traditional deep vision methods with the nuanced knowledge required to interpret images at such high semantic levels.

Objective: To bridge the gap between deep vision and situated perceptual paradigms, this study aims to establish a situated knowledge graph (KG) for a deeper understanding of abstract concept (AC) evocation in cultural images. Leveraging this knowledge, the objective is to enhance performance and interpretability in AC image classification.

Methods: We construct the ARTstract Knowledge Graph (AKG), capturing perceptual semantics from over 14,000 cultural images labeled with ACs. We extract perceptual semantic units using off-the-shelf models and integrate them into the AKG, enriching it with high-level linguistic frames. For AC-based image classification, we adopt a hybrid approach, integrating knowledge graphs and visual transformers. Specifically, we compute knowledge graph embeddings (KGE) on AKG and fuse them with visual transformer embeddings. For interpretability, we conduct post-hoc qualitative analyses by probing model similarities with training instances.

Results: Our hybrid methods outperform existing techniques in abstract concept (AC) image classification. Through post hoc interpretability analyses, we reveal the deep visual model's proficiency in capturing low-level visual attributes, contrasting with our method's efficacy in representing abstract and semantic scene elements.

Conclusion: Our results demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding, showcasing the potential of neuro-symbolic methods for robust image representation in intricate visual comprehension tasks. All the materials and code are available at https://anonymous.4open.science/r/Stitching-Gaps-B339/.",no,,,
18,2,sem24-research,Assessing the FAIRness of Software Repositories using RDF and SHACL,"Tobias Hummel, Leon Martin and Andreas Henrich",2024-04-17 07:02,2024-05-14 07:14,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/quare-8F4A
(Data Availability) https://anonymous.4open.science/r/quare-8F4A
(Ontology Availability) 
(ORKG) ","FAIR software
GitHub repositories
SHACL",accept,yes,yes,"Purpose: A previous paper proposed the usage of SHACL to assess the FAIRness of software repositories. Following this call to action, this paper introduces and discusses the changes made to QUARE, a SHACL-based tool for validating GitHub repositories against sets of quality criteria, to facilitate this task.
Methodology: An operationalization of the abstract FAIR best practices from previous work is devised to enable a FAIRness assessment based on concrete quality criteria. Afterwards, a SHACL shapes graph implementing these constraints is introduced, followed by a discussion of the efficient generation of suitable RDF representations for GitHub repositories. Improvements regarding the usability of QUARE are examined, as well. An evaluation on the FAIRness of 223 GitHub repositories and on the runtime performance of the assessment is conducted.
Findings: Trending repositories comply with fewer FAIR best practices than repositories expected to be FAIR on average. However, the latter still exhibit deficiencies, for example, regarding the correct application of semantic versioning. The low average runtime of the FAIRness assessment of respectively 3.94 and 6.20 seconds per repository permits the integration of QuaRe in, e.g., CI/CD pipelines.
Value: The FAIR principles are often mentioned as a measure to tackle the reproducibility crisis, which continues to have a significant impact on science. To implement these principles in practice, it is crucial to provide tools that facilitate the automated assessment of the FAIRness of software repositories. The enhanced version of QUARE introduced in this paper represents our proposal for this demand.",no,,,
19,2,sem24-research,A model and case study for searching and reading cross-border multilingual legislation on the Semantic Web,"Eero Hyvönen, Hien Cao, Rafael Leal, Heikki Rantala and Aki Hietanen",2024-04-19 09:03,1970-01-01 00:00,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) Code of the portal demonstrator and the Sampo-UI framework used in it are available with CC BY 4.0 license in GitHub:

https://github.com/orgs/SemanticComputing/repositories?type=all
(Data Availability) Linked Open Data service underlying the portal:

https://www.ldf.fi/dataset/finestlaw

The data are available with CC BY 4.0 license
(Ontology Availability) Open portal demonstrator is available on the Web:

https://finestlaw.demo.seco.cs.aalto.fi/en
(ORKG) ","Linked data
Law
Multilingual
Semantic portal
Data service",accept,yes,yes,"This paper concerns the problem of searching legislative documents in
an international cross-broader multilingual setting. Here, legal documents are orig-
inally published in different countries using different local languages, and the end-users search for the documents using their own languages. Furthermore, different
country-specific semantic keyword and classification systems for indexing the con-
tents may have been used. Cross-border services are needed, e.g., when moving
from one country to another and looking for regulations for immigration, heath
care, education, etc. To address the challenge, a cross-border solution based on
Linked Open Data and Semantic Web technologies is presented, and a proof-of-
concept system was designed and implemented, using consolidated laws of Finland
and Estonia and EU directives as a case study. The demonstrator includes a seman-
tic portal and a LOD service. Based on the so-called Sampo Model, the main nov-
elty of the FINESTLAWSAMPO demonstrator presented is the provision of hetero-
geneous cross-country, multilingual, distributed legal data through multiple appli-
cation perspectives for faceted searching and exploring the data as well as for data
analysis in legal informatics.",no,,,
24,2,sem24-research,Entity Linking with Out-of-Knowledge-Graph Entity Detection and Clustering using only Knowledge Graphs,Cedric Möller and Ricardo Usbeck,2024-04-22 14:59,1970-01-01 00:00,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/ELwithOOKGDetectionClustering-AABD/
(Data Availability) 
(Ontology Availability) 
(ORKG) ","entity linking
out-of-KG
nil entities",accept,yes,yes,"Entity Linking is crucial for numerous downstream tasks, such as question answering, knowledge graph population, and general knowledge extraction. A frequently overlooked aspect of entity linking is the potential encounter with entities not yet present in a target knowledge graph. Although some recent studies have addressed this issue, they primarily utilize full-text knowledge bases or depend on external information.
However, these resources are not available in most use cases. In this work, we solely rely on the information within a knowledge graph and assume no external information is accessible.

To investigate the challenge of identifying and disambiguating entities absent from the knowledge graph, we introduce a comprehensive silver-standard benchmark dataset that covers texts from 1999 to 2022. 
Based on our novel dataset, we develop an approach using pre-trained language models and knowledge graph embeddings without the need for a parallel full-text corpus.
Moreover, by assessing the influence of knowledge graph embeddings on the given task, we show that implementing a sequential entity linking approach, which considers the whole sentence, can outperform clustering techniques that handle each mention separately in specific instances.",no,,,
26,2,sem24-research,Securing Linked Data: Authorisation Ontology and Enforcement Mechanisms in the Dutch Federated Data System Context,"Alexandra Rowland, Hans Schevers, Erwin Folmer, Sven Mol, Janneke Michielsen and Marc van Andel",2024-04-22 16:13,2024-05-13 15:43,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://kadaster-labs.github.io/lock-unlock-docs/
https://github.com/kadaster-labs/lock-unlock-docs
(Data Availability) 
(Ontology Availability) 
(ORKG) ","Secured SPARQL Endpoints
Federated Querying
Authorization Ontology
Linked Data",accept,yes,yes,"To advance the adoption of linked data in the context of the Dutch Federated Data System (Dutch synonym: FDS), it is necessary to have robust access control for native linked data sources. For this purpose, research was initiated to assess whether it is feasible to implement access controls on linked data sources in this context. A four-phase design science research methodology is applied. The first phase defines both the question guiding this research and the context in which the research was conducted. The second phase includes a review of the state-of-the-art and an evaluation of the existing approaches to access control could support the FDS use case. Having determined that no existing approaches completely fulfil the requirements of the FDS use case, the third phase describes a prototype enforcement mechanism designed and developed as part of this research. The fourth and final phase evaluates this protype with respect to its feasibility to support the requirements of the FDS context. At present, there are no standardized solutions for securing native linked data sources. Existing literature and industry examples from the Netherlands and Europe highlight several potential solution directions for access control on SPARQL endpoints. These solution directions are used as inspiration for the development of a prototype enforcement mechanism. This prototype shows potential when applied to the Dutch Federated Data System and suggests a more generic approach could be taken when applying these controls to a broader context. Further research, testing and standardization efforts are required to bring such an approach to maturity. Any linked data ecosystem containing closed information requires a robust approach to access control. This research contributes to the existing literature on approaches taken to such access controls and highlights the increasing need for, and the feasibility of implementing, these controls in governmental contexts. Bringing such a solution to maturity would support wider adoption of linked data technologies in this context.",no,,,
30,2,sem24-research,Zero-shot Topic Classification of Column Headers: Leveraging LLMs for Metadata Enrichment,"Margherita Martorana, Tobias Kuhn, Lise Stork and Jacco van Ossenbruggen",2024-04-24 12:22,2024-05-13 21:47,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/LLMs-topic-classification-C681
(Data Availability) https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=80393eng&_theme=1068 ; Education expenditure and indicators
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=71950eng&_theme=1092 ; Health expectancy; since 1981
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=85538ENG&_theme=1160 ; Listed monuments; region 2023
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=84952ENG&_theme=1048 ; Livestock
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=7425eng&_theme=1131 ; Milk supply and dairy production
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=84710ENG&_theme=1176 ; Mobility per person, travel modes, travel purpose 
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=83566ENG&_theme=1133 ; Plant protection products; sales
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=83474ENG&_theme=1138 ; Population dynamics; month and year
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=37789eng&_theme=1104 ; Social security; key figures
https://opendata.cbs.nl/statline/portal.html?_catalog=CBS&_la=en&tableId=81156eng&_theme=1173 ; Trade and industry; finance, SIC 2008
(Ontology Availability) https://vocabularies.cessda.eu/vocabulary/TopicClassification ; 
CESSDA Controlled Vocabulary for CESSDA Topic Classification
(ORKG) https://orkg.org/paper/R703708/R703713","Large Language Models
Metadata
FAIR Guiding Principles
Retrieval Augmented Generation
Linked Data",accept,yes,yes,"Traditional dataset retrieval systems rely on metadata for indexing, rather than on the underlying data values. However, high-quality metadata creation and enrichment often require manual annotations, which is a labour-intensive and challenging process to automate. In this study, we propose a method to support metadata enrichment using topic annotations generated by three Large Language Models (LLMs): ChatGPT-3.5, GoogleBard, and GoogleGemini. Our analysis focusses on classifying column headers based on domain-specific topics from the Consortium of European Social Science Data Archives (CESSDA), a Linked Data controlled vocabulary. Our approach operates in a zero-shot setting, integrating the controlled topic vocabulary directly within the input prompt. This integration serves as a Retrieval-Augmented Generation (RAG) approach, with the aim of improving the results of the topic classification task.

We evaluated the performance of the LLMs in terms of internal consistency, inter-machine alignment, and agreement with human classification. Additionally, we investigate the impact of contextual information (i.e., dataset description) on the classification outcomes. Our findings suggest that ChatGPT and GoogleGemini outperform GoogleBard in terms of internal consistency as well as LLM-human-alignment. Interestingly, we found that contextual information had no significant impact on LLM performance. 

This work proposes a novel approach that leverages LLMs for topic classification of column headers using a controlled vocabulary, presenting a practical application of LLMs and RAG systems within the Semantic Web domain. This approach has the potential to facilitate automated metadata enrichment, thereby enhancing dataset retrieval and the Findability, Accessibility, Interoperability, and Reusability (FAIR) of research data on the Web.",no,,,
32,5,sem24-industry,Linked Data Event Streams for cultural heritage networks,"Pieter Colpaert, Gertjan Filarski and Anastasia Sofou",2024-04-29 14:29,1970-01-01 00:00,,"Linked Data Event Streams
Cultural Heritage
CIDOC
Linked Art",accept,no,no,"It is not uncommon for European organizations to experience the need to aggregate data from multiple sources into one for the purpose of having a pan-European overview, or to create specific thematic subsets. By publishing a Linked Data Event Stream (LDES) of an art collection, we enable everyone, whether you’re Europeana, an archive, or a thematic project such as “Against Opacity”,  to fully replicate the art collection and publish a derived version that is guaranteed to stay in sync with the source thereafter. In this talk, we present Interoperable Europe’s SEMIC pilot for the adoption of LDES with CIDOC-CRM at Rijksmuseum and how that enables us now to reuse existing generic RDF Connect tooling. That tooling was already employed in completely distinct pilots as well, such as address registries with the SEMIC Core Vocabularies, metadata harvesting with DCAT-AP, marine biology taxonomies from Maregraph, or the European railway infrastructure of ERA.",no,,,
35,2,sem24-research,UniPart: Optimizing Streaming Graph Partitioning towards Universal Adaption in RDF Triple Stores,"Wenhui Yang, Ahmed Al-Ghezi and Lena Wiese",2024-05-01 18:27,1970-01-01 00:00,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/wenhuiyang23/UniPart
(Data Availability) 
(Ontology Availability) 
(ORKG) ","RDF
Workload Awareness
Triple Store
Graph Partitioning",accept,yes,yes,"Purpose:    
With increasing size of Resource Description Framework (RDF) graphs, the resulting graph structures can become too large to be managed on a single compute node, lacking the necessary resources to execute a partitioning of the graph -- in particular, when the partitioning method relies on global graph information for which the entire graph has to be loaded into the main memory. This paper introduces a window-based streaming partitioning technique to obtain distributed RDF graphs, overcoming the memory limitations of traditional partitioning methods.
Methodology:
We evaluated our approach, UniPart, by comparing it with established graph partitioning algorithms such as METIS, LDG, and WStream. The comparison focused on key metrics, including the proportion of edge cuts.
Findings:
Through practical assessments using the LUBM dataset, our algorithm demonstrated strong performance in load balance, execution time, and memory usage. Notably, under the DFS streaming order, UniPart achieved a 20\% reduction in edge-cut ratio compared to LDG.
Value:
UniPart operates without the need for global graph information, making it exceptionally suited for dynamic environments with unbounded streams and unpredictable data sizes. 



",no,,,
38,2,sem24-research,Semantic Smart Readiness Indicator Framework,"Stefan Bischof, Erwin Filtz and Josiane Xavier Parreira",2024-05-03 08:29,2024-05-13 09:10,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/sri-B22D/README.md
(Data Availability) https://anonymous.4open.science/w/sri-B22D/OnToology/sri.1.0.ttl/documentation/index-en.html
(Ontology Availability) 
(ORKG) ","Smart Readiness Indicator
Smart building
Ontology engineering",accept,yes,yes,"Purpose: The Smart Readiness Indicator (SRI) is an energy rating scheme
targeted at buildings to evaluate their capacity to integrate and benefit from smart
technologies for enhanced energy efficiency and overall performance. Existing tools
for SRI assessment and rating do not provide a standard format for data exchange.
However, there are several scenarios in which a FAIR, standardised data format is
beneficial, such as data exchange between building tools, comparison of different
assessments, or computing statistics about buildings.
Methodology: We propose the Semantic Smart Readiness Indicator framework,
consisting of an SRI information model and a SPARQL-based SRI score calculation.
We follow the Linked Open Terms ontology engineering method by specifying the
use case from which the requirements and competency questions are derived. We
reuse existing ontologies and extend them to create the SRI ontology. Findings:
The model is published according to the FAIR principles. Moreover, it is flexible to
accommodate specific SRI requirements, and can be aligned with existing semantic
building models to facilitate data linking and exchange. The score calculation, in
turn, is composed of multiple SPARQL queries defined over the model.
Value: In this paper, we describe our proposed framework, the ontology engineering
process, and the evaluation of both the model and the SPARQL-based SRI calculation. All the resources are openly available for reuse.",no,,,
50,2,sem24-research,Representing and searching associations in cultural heritage knowledge graphs using faceted search,"Heikki Rantala, Petri Leskinen, Lilli Peura and Eero Hyvönen",2024-05-05 20:03,2024-05-13 20:39,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) https://intaviasampo.demo.seco.cs.aalto.fi/en/ ; a demonstrator
https://intapedia.demo.seco.cs.aalto.fi/en/ ; a demonstrator
https://ulansampo.demo.seco.cs.aalto.fi/en/ ; a demonstrator
(ORKG) ","Relational search
Association search
Cultural heritage
Biographies
Knowledge graphs",accept,yes,yes,"This paper presents how relations or associations between entities such as persons in cultural heritage knowledge graphs can be searched and analyzed using faceted search and visualizations. Faceted search using well formed ontologies allows search and comparison of relative numbers in associations of groups of entities, such as artists from different countries, and reveal patterns in the data. This papers presents examples of how this can be done in practice, and how the associations can be conceptualized in different ways that affect the performance of the search, and how the associations can be analyzed. The concept of faceted association  search is examined in this paper through case studies including searching relations in Finish and European Biographies, relations in Union List of Artist Names (ULAN), and relations formed by links between Wikipedia pages of persons.",no,,,
53,2,sem24-research,TWIG-I: Embedding-Free Link Prediction and Cross-KG Transfer Learning using a Small Neural Architecture,"Jeffrey Sardina, Alok Debnath, John D. Kelleher and Declan O'Sullivan",2024-05-05 20:44,2024-05-13 17:03,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/TWIGIB6B5/
(Data Availability) https://anonymous.4open.science/r/TWIGIB6B5/ ; KG data in code repo via the PyKEEN library
(Ontology Availability) 
(ORKG) ","Link Prediction
Knowledge Graph Embeddings
Graph Topology",accept,yes,yes,"Knowledge Graphs (KGs) are relational knowledge bases that represent facts as a set of labelled nodes and the labelled relations between them. Their machine learning counterpart, Knowledge Graph Embeddings (KGEs), learn to predict new facts based on the data contained in a KG -- the so-called link prediction task. To date, almost all forms of link prediction for KGs rely on some form of embedding model, and KGEs hold state-of-the-art status for link prediction. In this paper, we present TWIG-I (Topologically-Weighted Intelligence Generation for Inference), a novel link prediction system that can represent the features of a KG in latent space without using node or edge embeddings. We show that TWIG-I can increase performance on the link prediction relative to KGE models, including a 35 base-point increase in MRR performance on FB15k-237 over the strongest baseline; this represents a 100% relative increase in performance. Unlike KGEs, TWIG-I can be natively used for transfer learning across KGs, even across KGs that come from different knowledge domains. We show that using transfer learning with TWIG-I can lead to notable increases in performance both over KGE baselines and over TWIG-I models trained without finetuning. With finetuning, TWIG-I is able to achieve a 44 base-point increase in MRR over the standard benchmark KG FB15k-237 relative to the strongest baseline, which corresponds to a 126% relative increase in predictive performance.",no,,,
55,2,sem24-research,Historical Opera and Music Theatre Performances on the Semantic Web: OperaSampo 1830–1960,"Annastiina Ahola, Eero Hyvönen, Heikki Rantala and Anne Kauppala",2024-05-06 05:28,2024-05-13 08:06,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) Code openly available:

https://anonymous.4open.science/r/operasampo-web-app-E833/
(Data Availability) OperaSampo data and LOD service openly availabe at:

https://www.ldf.fi/dataset/operasampo
(Ontology Availability) The OperaSampo portal available online in English and Finnish at: https://oopperasampo.fi/en/
(ORKG) ","Cultural Heritage
Linked Data
User Interfaces
Portals",accept,yes,yes,"The OperaSampo is a Linked Open Data (LOD) service and semantic portal for searching, browsing, and analyzing information related to historical opera and music theatre performances performed in Finland during 1830–1960. The key data originates from the Reprises database of the Sibelius Academy, Finland. This paper presents the process of transforming the original data into LOD and the data model created for it, data maintenance, as well as the portal and data service for utilizing the data. The novelty of OperaSampo lays on its focus on studying data about the musical performances and persons involved in different roles using faceted search and browsing combined seamlessly with data-analytic tools for Digital Humanities research. The service was published for open use in October 2023.",no,,,
59,2,sem24-research,Exploring  Prompt Generation Utilizing Graph Search Algorithms for Ontology Matching,"Julian Sampels, Sefika Efeoglu and Sonja Schimmler",2024-05-06 08:29,2024-05-14 00:30,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/OntoMatch-SemanticsConf
(Data Availability) https://oaei.ontologymatching.org/2023/conference/index.html ; Conference track
https://owl.vse.cz/ontofarm/ ; OntoFarm
(Ontology Availability) 
(ORKG) ","Graph Search
Prompt Generation
Ontology Matching
Zero-Shot Prompting",accept,yes,yes,"The interoperability of domain ontologies, developed by domain experts, necessitates their alignment before attempting to match them. Within these ontologies, defined concepts often encounter an ambiguity problem stemming from the use of natural language. This interoperability issue raises the underlying ontology matching (OM) challenge. OM might be defined as the identification of correspondences or relationships between two or more entities, such as classes or properties among two or more ontologies. Rule-based ontology matching approaches, e.g., LogMap and AML have not outperformed machine learning based matchers on the Ontology Alignment Evaluation Initiative (OAEI) benchmark datasets, especially on the OAEI Conference Track since 2020. Supervised machine or deep learning approaches produce the best results but require labeled training datasets. In the era of Large Language Models (LLMs), robust zero-shot prompting of LLMs can also return convincing responses. While prompt generation requires prompt template engineering by domain experts, contextual information about the concepts to be aligned can be retrieved by leveraging graph search algorithms. In this work, we explore how graph search algorithms, namely (i) random walk and (ii) tree traversal can be utilized to retrieve the contextual information to be incorporated into prompt templates. Through these algorithms, our approach refrains from considering all triples connected with a concept to be aligned in its contextual information creation. Our experiments show that including the retrieved contextual information in prompt templates improves the matcher's performance. Additionally, our approach outperforms previous works leveraging zero-shot prompting.",no,,,
60,2,sem24-research,Semantically Describing Predictive Models for Interpretable Insights into Lung Cancer Relapse,"Yashrajsinh Chudasama, Disha Purohit, Philipp D. Rohde, Enrique Iglesias, Maria Torrente and Maria-Esther Vidal",2024-05-06 08:35,2024-05-14 08:55,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://figshare.com/s/442d575a8e88d450ea85
(Data Availability) 
(Ontology Availability) 
(ORKG) ","Knowledge Graphs
Machine Learning
Interpretability",accept,yes,yes,"Machine learning (ML) is becoming increasingly important in healthcare decision-making, requiring highly interpretable insights from predictive models. Although integrating ML models with knowledge graphs (KGs) holds promise, conveying model outcomes to domain experts remains challenging, hindering usability despite accuracy. We propose semantically describing predictive model insights to overcome communication barriers. Our pipeline predicts lung cancer relapse likelihood, providing oncologists with patient-centric explanations based on input characteristics. Consequently, domain experts gain insights into both the characteristics of classified lung cancer patients and their relevant population. These insights, along with model decisions, are semantically described in natural language to enhance understanding, particularly for interpretable models like LIME
and SHAP. Our approach, SemDesLC, documents ML model pipelines into KGs, and fulfills the needs of three types of users: KG builders, analysts, and consumers. Experts’ opinions indicate that semantic descriptions are effective for elucidating relapse determinants. SemDesLC is openly accessible on Figshare, promoting transparency and collaboration in leveraging ML for healthcare decision support.",no,,,
61,2,sem24-research,Overlap and Quality Aware Query Processor for Federations of Triple Fragment Interfaces,"Tobias Zeimetz, Katja Hose and Ralf Schenkel",2024-05-06 09:33,2024-05-14 07:48,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/anonresearcher123/ORAQL
(Data Availability) https://shorturl.at/fN067
(Ontology Availability) No additional sources
(ORKG) No ORKG comparisions","Overlapping Data Sources
Reliability
TPF Interface
Federation",accept,yes,yes,"The increasing numbers of available data sources have led to increased data redundancy and hence novel challenges for federations. Typically, federation engines query all endpoints that provide relevant data for a given query. However, considering the overlap, a subset of these sources might already be sufficient to obtain a complete answer. Further, we deliberately might not wish to include all sources in the evaluation and make a decision based the reliability of a source. We therefore present ORAQL (an Overlap and Reliability Aware Query Processing Layer), an approach that exploits statistics capturing the overlap between sources to choose a subset of the available sources in the federation to compute a complete answer while minimizing redundant answers. Moreover, a user-provided reliability goal is taken into account. Hence, we propose an approach based on a majority vote over multiple sources to increase the reliability of the query result. For this work, we focus on TPF interfaces, since they are the least expressive interfaces and hence our approach can easily be adopted for more expressive interfaces, e.g. SPARQL endpoints. The presented methods to capture the overlap between sources of a federation have shown to generate useful overlap profiles with a maximum deviation of less than five percent. Even if the identification of redundant data is NP-hard we presented an approximation with a significant reduction in requested endpoints. Further, we have shown that ORAQL is granularly tunable towards reliability and can beat a state-of-the-art baseline system in terms of coverage and reliability.",no,,,
63,2,sem24-research,ReWise: A Relation-Wise Sampling Framework for Relational Graph Convolutional Networks,"Taraneh Younesian, Peter Bloem and Stefan Schlobach",2024-05-06 10:29,2024-05-14 10:49,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/ReWise-7735
(Data Availability) https://github.com/pbloem/kgbench-data/tree/main; kgbench graphs. amplus, dmgfull, mdgenre
(Ontology Availability) 
(ORKG) ","Machine learning for knowledge graphs
Relational graph convolutional network
Scalability
Sampling",accept,yes,yes,"Relational graph convolutional networks (RGCNs) have been successful in learning from knowledge graphs. However, training on large-scale knowledge graphs becomes challenging due to the exponential growth of the neighborhood size across the network layers. Moreover, knowledge graphs have multiple relations, and often, the literals can have multimodal content; these properties make it extra challenging to scale up the training of RGCNs to large-scale graphs. Graph sampling techniques have been shown to be effective in scaling learning to large graphs by reducing the number of processed nodes and lowering memory usage. However, only a few studies have focused on sampling for knowledge graphs. In this work, we introduce ReWise, a relation-wise sampling framework that includes a family of sampling methods designed for knowledge graphs. Our experiments demonstrate that sampling reduces memory usage up to 50% lower than the case without sampling while maintaining the same classification accuracy and, in some cases, outperforming it. Additionally, we show that our sampling strategy is compatible with the multimodal RGCN, showing the same behavior as RGCNs.",no,,,
64,2,sem24-research,Towards Digital Sustainability Reporting: An Ontology for Mapping of Indicators in GRI and ESRS,"Yuchen Zhou, Yuan Cao and Alexander Perzylo",2024-05-06 10:35,2024-05-14 11:54,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/tesmachina/RSO
(Data Availability) https://github.com/tesmachina/RSO/RSO-examples
(Ontology Availability) https://tesmachina.github.io/rso.github.io/
https://github.com/tesmachina/RSO/ontology-file
(ORKG) ","Sustainability Reporting
Environmental Indicator
Ontology Engineering
Semantic Interoperability
Standards Mapping",accept,yes,yes,"Sustainability reporting by Small and Medium Enterprises (SMEs) is gaining importance. SMEs form the backbone of European industries, and their customers rely on them to ensure regulatory compliance. In preparing sustainability reports, a combination of standards is commonly used, which encompasses overlapping yet distinct requirements on sustainability indicators. Different standards categorize shared indicators under varying topics, while they also mandate unique indicators to assess identical sustainability phenomena. This poses challenges for SMEs in reporting against multiple standards. Considerable human efforts are demanded to determine the interconnected requirements across different standards. Additionally, reporting on overlapping indicators for new standards results in significant redundant work. Mapping of indicators between different standards allows the semantic interoperability of standards by indicating matching and distinct requirements, aiding in addressing these challenges. Therefore, this paper focuses on developing an ontology for mapping indicators from two significant standards, GRI and ESRS. We introduce the Sustainability Reporting Standards Ontology (RSO). RSO formally represents environmental indicators in GRI and ESRS, emphasizing indicator requirements such as unit, quantity, and measurement variables. RSO is implemented in the RDF/OWL format and will be made available online. Furthermore, we provide an ontology-based mapping between indicators, supported by concrete examples that illustrate the interconnections between them.",no,,,
65,2,sem24-research,Towards Efficient Exploitation of Large Knowledge Bases by Context Graphs,Nada Mimouni and Jean-Claude Moissinac,2024-05-06 11:17,2024-05-13 19:42,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://gitlab.com/-/snippets/3709662; A selected list of nodes to be excluded from context graphs
(Data Availability) https://givingsense.eu/data/datamusee/kore/korecg/; KORE context graph
(Ontology Availability) 
(ORKG) ","Knowledge base
Context graph
Similarity
DBpedia
Joconde database",accept,yes,yes,"One problem related to the exploitation of knowledge graphs, in particular when processing with machine learning methods, is the scaling up problem. We propose here a method to significantly reduce the size of the used graphs to focus on a useful part in a given usage context. We define the notion of context graph as an extract from one or more general knowledge bases (such as DBpedia, Wikidata, Yago) that contains the set of information relevant to a specific domain while preserving the properties of the original graph. We validate the approach on a DBpedia excerpt for entities related to the Data\&Musée project and the KORE reference set according to two aspects: the coverage of the context graph and the preservation of the similarity between its entities. The results show that the use of context graphs makes the exploitation of large knowledge bases more manageable and efficient while preserving the properties of the initial graph.",no,,,
67,2,sem24-research,Enhancing Answers Verbalization using Large Language Models,"Daniel Vollmers, Parth Sharma, Hamada Zahera and Axel-Cyrille Ngonga Ngomo",2024-05-06 12:16,2024-05-14 10:53,"(Paper Type) Short
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://zenodo.org/records/11185539
(Data Availability) 
(Ontology Availability) https://zenodo.org/records/11185539 ; trained models and outputs
(ORKG) ","KGQA
Verbalization
Question Answering",accept,yes,yes,"Purpose: This study investigates the verbalization of answers generated by knowledge graph question answering (KGQA) systems using large language models. In user-centric applications, such as dialogue systems and voice assistants, answer verbalization is an essential step to enhance the quality of interactions.
Methodology: We experimented with multiple large language models to verbalize answers from knowledge-based question-answering systems. In particular, we fine-tuned the LLM models based on different inputs, including SPARQL queries and triples to determine which model performs best for answer verbalization.
Findings: We found that fine-tuning language models and introducing additional knowledge such as SPARQL queries, achieve state-of-the-art results in verbalizing answers from KGQA systems.
Value: Our approach can be used to generate verbalizations of answers from different kinds of KGQA systems for dialogue systems or voice assistants.",no,,,
75,2,sem24-research,Investigate the Impact of Contextual Information on LLMs for Taxonomy Expansion,"Artem Revenko, Anna Breit, Salma Mahmoud, Mark Szabo and Tomas Knap",2024-05-06 14:18,2024-05-14 06:03,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/te_experiments-7DB5
(Data Availability) 
(Ontology Availability) 
(ORKG) https://doi.org/10.48366/R722259","Large Language Models
Prompt Engineering
Taxonomy Expansion",accept,yes,yes,"This paper presents an exploratory study that investigates the use of various Large Language Models (LLMs) for the task of taxonomy expansion. 
Our objective is to enhance the taxonomical structure by querying LLMs for (1) child taxons and (2) alternative labels of existing taxons. Beginning with an incomplete taxonomy, we explore the most effective ways to prompt LLMs exploiting explicit and shared knowledge captured in manually curated taxonomies to provide context for the task at hand. We experiment with different prompting templates, well-recognized taxonomies (EuroVoc, STW, UNESCO), and popular language models (Claude, Claude3, Llama2).
Our results suggest feasibility of solving of the proposed task with the modern LLMs and human oversight. Moreover, we observe certain patterns and trends in the performance of the models, noting that it was not possible to identify a single best configuration that would fit all models.",no,,,
76,2,sem24-research,Generating Sparql from Natural Language Using Chain-of-Thoughts Prompting,"Hamada Zahera, Manzoor Ali, Mohamed Ahmed Sherif, Diego Moussallem and Axel-Cyrille Ngonga Ngomo",2024-05-06 14:55,2024-05-13 20:43,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(ORKG) ","SPARQL Generation
Large Language Models
Chain of Thoughts Prompting",accept,yes,yes,"Purpose:
Sparql is a highly expressive query language for knowledge graphs; yet, formulating precise Sparql queries can be challenging for users non-expert users. A potential solution is translating natural questions into Sparql queries, known as Sparql generation. This paper addresses the challenges of translating natural language questions into Sparql queries for different knowledge graphs.

Methodology: 
We propose CoT-Sparql, our approach to generate Sparql queries from input questions. Our approach employs Chain-of-thoughts prompting that guides large language models through intermediate reasoning steps and facilitates generating precise Sparql queries. Furthermore, our approach incorporates entities and relations from the input question, and one-shot example in the prompt to provide additional context during the query generation process.

Findings: 
We conducted several experiments on benchmark datasets and showed that our approach outperforms the state-of-the-art methods by a large margin. Our approach achieves a significant improvement in F1 score of 4.4% and 3.0% for the QALD-10 and QALD-9 datasets, respectively.

Value: 
Our CoT-Sparql approach contributes to the semantic web community by simplifying access to knowledge graphs for non-expert users. In particular, CoT-Sparql enable non-expert end-users to query knowledge graphs in natural languages, where CoT-Sparql converts user natural languages queries into Sparql queries, which can be executed via the knowledge graph’s Sparql endpoint.",no,,,
77,2,sem24-research,Leveraging Semantic Model and LLM for Bootstrapping a Legal Entity Extraction: An Industrial Use Case,"Julien Breton, Mokhtar Boumedyen Billami, Max Chevalier and Cassia Trojahn",2024-05-06 14:56,2024-05-13 15:43,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://gitlab.irit.fr/ala/legal-entity-extraction
(Data Availability) 
(Ontology Availability) 
(ORKG) ","Legal Entity Extraction
Semantic Model
Large Language Models",accept,yes,yes,"Compliance with legal documents related to industrial maintenance is the company's obligation to oversee, maintain, and repair its equipments. As legal documents endlessly evolve, companies are in favour of automatically processing these texts to facilitate the analysis and compliance. The automatic process involves first, in this pipeline, the extraction of legal entities. However, state-of-the-art approaches, like rule-based, Bi-LSTM or BERT for legal entity extraction have so far required a sufficient amount of data to be effective. Creating these training dataset however is a time-consuming task requiring input from domain experts. In this paper, we bootstrap the legal entity extraction by levering Large Language Models and a semantic model in order to reduce the involvement of the domain experts. We develop the industrial perspective by detailing the technical implementation choices. Consequently, we present our roadmap for an end-to-end pipeline designed expressly for the extraction of legal rules while limiting the involvement of experts.",no,,,
79,2,sem24-research,A Foundational Ontology of Deepfake Attacks and Knowledge Graph Application,Faiza Khalid and Oğuzhan Menemencioğlu,2024-05-06 15:49,2024-05-13 20:57,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) 
(Data Availability) https://anonymous.4open.science/r/DeepfakeAttackKnowledgebase/
(Ontology Availability) https://anonymous.4open.science/r/DeepfakeOntology/
https://anonymous.4open.science/r/DFOntologyDocumentation-022C/
(ORKG) ","deepfakes
ontology development
knowledge graphs",accept,yes,yes,"Purpose: The purpose of this paper is to develop the first deepfake domain ontology that could assist common individuals in understanding the growing concerns of AI-manipulated digital media and researchers in domain knowledge integration and inference. For a foundational ontology, authors focused on structuring knowledge related to a deepfake attack, like the vulnerable entity, deepfake creator, attack goal, medium, generation technique, consequences, preventive measures, etc. The authors implemented knowledge graphs to evaluate ontology’s effectiveness in helping understand and infer deepfake event data.

Methodology: The authors used knowledge engineering methodology with 7 steps, including ontology scope determination, existing ontologies evaluation, and classes, properties, and relations definitions. The authors utilized Protégé Desktop and the W3C Web Ontology Development Language for ontology creation, the WIDOCO tool for ontology documentation, and OOPS for ontology validation. The authors developed a small-size deepfake events knowledge base to implement knowledge graphs, where the developed ontology defined the nodes and relations. GraphDB, a graph database, was used for knowledge graph implementation.

Findings: The manual literature review from prominent research publications, like IOS Press, IEEE Proceedings, CEUR-WS, etc., and evaluation of existing ontologies like DBpedia, MLOnto, and SEiCS helped identify 19 core entities and 28 relations describing the deepfake domain. The authors created SWRL rules that helped infer additional information from the deepfake attack knowledgebase via knowledge graphs application, such as various ways a particular entity can be affected by a deepfake, mediums used for attacks, and online security measures victims can adopt.

Value: Advanced AI-based Deepfakes are a threat to social, political, and economic structure via cases of defamation, political influence, financial fraud, harassment, etc. The developed ontology could be used to promote domain understanding and as a framework to build cybersecurity systems with better knowledge inference (semantic reasoning). The ontology can be extended iteratively with new domain advancements. As a next step, we plan on adopting NLP approaches for automating domain entity research and deepfake event knowledge base population.",no,,,
82,2,sem24-research,Ontology based Event Knowledge Graph enrichment using case based reasoning,"Rajesh Piryani, Nathalie Aussenac-Gilles, Nathalie Hernandez, Cédric Lopez and Camille Pradel",2024-05-06 16:27,2024-05-14 07:53,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/xpEventCore-065C
(Data Availability) 
(Ontology Availability) 
(ORKG) ","Event Knowledge Graph
5W1H
Event Ontology
Case-based reasoning
Knowledge graph enrichment",accept,yes,yes,"Purpose: The first objective of this research is to represent an event with 5W1H characteristics (who, what, where, when, why, and how) through ontologies. The second objective is to propose an approach for enriching an event knowledge graph (EKG) based on this ontology using EvCBR, a outperforming case-based reasoning algorithm found in the literature. Furthermore, we have studied the impact of each W (Who, Where, and When) on the performance of EvCBR on the Wikipedia Causal Event dataset. 

Methodology:  We proposed the XPEventCore ontology to represent 5W1H characteristics of events by integrating multiple event ontologies (SEM and FARO) and introduced new object properties for representing Cause and Method to answer “Why” and “How” questions. We adopted this XPEventCore ontology for a specific use case (the MR4AP Wikipedia dataset), and populated our EKG. Furthermore, we adapted EvCBR, a case-based reasoning approach, to enrich this EKG.

Findings: XPEventCore ontology provides a structured and adaptable foundation for capturing the essential facets of an event. It can be adapted to any domain (like MR4AP Wikipedia dataset) and populated to generate EKGs. Then, we applied the EvCBR, and subsequent analysis revealed that reasoning had a significant impact. Notably, the EvCBR outperformed both the with and without reasoning approaches on the dataset. 

Originality: XPEventCore ontology is the first ontology that represents an event with 5W1H characteristics.",no,,,
84,2,sem24-research,What do we Annotate when we Annotate? Towards a Multi-Level Approach to Semantic Annotations,"Maria Francesca Bocchi, Carlo Teo Pedretti, Francesca Tomasi and Fabio Vitali",2024-05-06 16:42,2024-05-13 22:23,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/mirador-multi-level-annotations-3079
(Data Availability) 10.5281/zenodo.11188142
(Ontology Availability) 
(ORKG) ","Semantic Annotations
Digital Images
Cultural Heritage
Knowledge Representation
Digital Hermeneutics",accept,yes,yes,"Purpose: Annotating is considered a 'scholarly primitive' among different fields in the humanities. Nevertheless, the debate on digital annotations has mostly focused on the annotation of textual data, whereas existing models for representing annotations of images still lack sufficient semantic richness to anchor the annotation itself to multiple conceptual levels. We address the challenge of defining a data model to overcome the problem of ‘semantic deficit’ in this application domain. Finally, we implement an annotation client for testing multi-level semantic annotations. 

Methodology: To define a data model for representing digital annotations, we analysed applications which support annotation images through IIIF protocol, focusing on digital representations of palimpsests. We then extended the Web Annotation Data Model by introducing domain standards such as LRMer, CIDOC-CRM, and HiCo. We also validated the model through SPARQL queries corresponding to five competency questions to report on satisfiability. Finally, we developed a prototype annotation client as a plugin for Mirador to evaluate its performances in real-world scenarios. 

Findings: The results indicate that our model can effectively disambiguate between a target image and multiple conceptual levels of the entity itself, proving to be decisive in the representation of entities that coexist in the same material item (e.g., palimpsests). Additionally, the model allows users to describe annotations as interpretative acts, incorporating scholarly criteria and multiple viewpoints. An interface plugin enables scholars without technical expertise to create structured annotations that comply with the model. 

Value: The proposed approach facilitates the detailed management of the relationships between digital resources and their annotations, improving interoperability and information accessibility in the Semantic Web domain. Future developments will concern further extensions of the model, considering information about versioning, provenance, and authoritativeness of the digital annotations on images, as well as support for meta-annotations and iconological levels of interpretation.",no,,,
87,2,sem24-research,An Ontological Framework for Integrating the Heterogeneous Medieval Manuscript Resources: A Case Study of Progetto Irnerio and Mosaico,Faria Ferooz and Monica Palmirani,2024-05-06 19:46,2024-05-13 21:43,"(Paper Type) Full
(Student) Yes
(Publication Resources) Yes
(Code Availability) https://figshare.com/s/37cb2af111d2c5dc2230
(Data Availability) 
(Ontology Availability) 
(ORKG) https://doi.org/10.48366/R721525","Data Integration
Digital Resource Collection
Medieval Manuscript Data Integration Ontology (MMDIO)
MeLOn Methodology
Heterogeneous Platforms
Semantic Richness",accept,yes,yes,"""Purpose: This study aimed to improve the organization and integration of heterogeneous medieval manuscript data across the Mosaico and Progetto Irneiro platforms. It addresses the challenges, such as the need for more standardization in data formats, metadata schemas, and inconsistent data quality, by developing a new ontology that supports the multifaceted analysis of medieval manuscripts. This analysis includes factors such as the historical context, physical characteristics, textual information, and artistic features.

Methodology: The MeLOn methodology is used to develop the Medieval Manuscript Data Integration Ontology (MMDIO), which extends the MeMO ontology. This process involves analyzing existing structures, identifying gaps, integrating elements from other ontologies, and creating new data classes and relationships. Protégé was used to design and validate schemas, GraphDB to test functionality and interoperability, Graffoo to visualize data, and LODE for publishing. Consequently, this comprehensive process significantly enhanced the data integration for the Mosaico and Irneiro systems.

Findings: The use of MMDIO substantially enhanced the organization, accessibility, and uniformity of metadata formats on both platforms. However, it can now handle complex queries and integrate multiple types of manuscript data to facilitate a more comprehensive and organized approach in medieval manuscript research.

Value: The proposed MMDIO framework advances digital humanities research by increasing data semantic richness and interoperability, establishing the foundation for future research in cultural heritage preservation. Moreover, it demonstrates the value of adapted frameworks for handling complex data environments in particular research fields.""",no,,,
88,2,sem24-research,Enabling Delayed-Answer Auctions for RDF Knowledge Graphs Monetisation,"Hala Skaf-Molli, Pascal Molli, Luis-Daniel Ibanez and Abraham Bernstein",2024-05-06 20:14,2024-05-14 05:41,"(Paper Type) Full
(Student) 
(Publication Resources) Yes
(Code Availability) https://anonymous.4open.science/r/auction-pap-E464/README.md
(Data Availability) 
(Ontology Availability) 
(ORKG) https://doi.org/10.48366/R722823","Web of Data
Data Monetisation
Auction models
Knowledge Graphs
Query Processing",accept,yes,yes,"Traditionally, querying knowledge graphs is free of charge, however, ensuring availability of data and service incurs costs to knowledge graphs
providers. The Delayed-Answer Auction (DAA) model has been proposed to fund the maintenance of knowledge graphs endpoints, by allowing customers to sponsor entities in the Knowledge Graph so query results that include them are delivered in priority. However, implementing DAA with a time to first results acceptable for data consumers is challenging because it requires reordering results according to bid values.
 In this paper, we present an approach to enable DAA with low impact on query execution performance.
Our approach relies on (i) reindex sponsored entities by bid value to ensure they are processed first (ii) Web preemption to ensure delayed answering. Experimental results demonstrate that our approach significantly outperforms a baseline execution in terms of time to deliver the first results.",no,,,
107,5,sem24-industry,Data-driven Energy-efficient Manufacturing at Dell Technologies’ Ireland Campus,Aidan O Mahony,2024-05-23 15:43,1970-01-01 00:00,,"Energy-efficient Manufacturing
Distributed Knowledge Graph
Swarm Intelligence for Distributed Search",accept,no,no,"Dell Technologies presents an industry use case focused on enhancing energy efficiency in manufacturing through advanced data analytics and semantic technologies, as part of the Horizon Europe funded GLACIATION project. Situated at the Cork (Ireland) Campus Manufacturing facility, this initiative analyzes data generated by Collaborative Robots (Cobots) and Autonomous Mobile Robots (Tugbots), integrating their operational and diagnostic data via the GLACIATION platform. The platform leverages a distributed knowledge graph, swarm intelligence for distributed search, and semantic web technologies as a metadata fabric to optimize the use of renewable energy sources, minimize energy consumption, and improve operational efficiencies. This innovative approach addresses the fragmented analysis of data from various robots, which previously hindered the facility's ability to optimize energy consumption and leverage advanced analytics.

By implementing the GLACIATION platform, Dell Technologies has created a data management and retrieval system, enhancing data accessibility and operational efficiency. The use of edge computing reduces latency, enabling real-time decision-making, while machine learning techniques extract valuable insights, facilitating predictive analytics and optimization of energy usage. The semantic reasoning capabilities further support data-to-knowledge transformation, ensuring effective resource management and operational efficiencies.

This use case showcases the practical application of these advanced technologies but also demonstrates significant benefits such as enhanced data retrieval efficiency, reduced operational costs, improved decision-making, and sustainability through effective utilization of renewable energy sources. Our presentation will detail the initial challenges, the innovative approach undertaken, the tangible benefits realized, and future prospects, providing a comprehensive view of the impact of semantic technologies in a real-world manufacturing setting.",no,,,
108,5,sem24-industry,Semantic Coupling of Digital Twins to derive Greenhouse Control Strategies,"David de Best, Klaas Andries de Graaf, Jack Verhoosel, Paolo de Heer, Yvon Gankema, Seymour Lubbers and Marcel van Vliet",2024-05-24 10:58,2024-05-28 11:57,,"greenhouse
digital twin
interoperability
common ontology
semantic alignment
graph patterns",accept,no,no,"One of the major challenges for growing crops in a greenhouse is determining which control strategy to use. Various digital twins exist that independently can simulate the effects of indoor climate- and crop-strategies on costs in terms of energy and resource usage and on benefits in terms of production and yield. Specifically, there are tools that can be used to generate growing strategies, tools to simulate greenhouse climate strategies and tools to calculate plant development. 
 
In the Dutch CODIT (Coupling Digital Twins) project, we combine and couple these digital twins in a semantically unambiguous manner using the Common Greenhouse Ontology (CGO), the standard for greenhouse data sharing. The value for greenhouse growers and advisors lies in better predictions on required climate, energy consumption, and production by combining the digital twins and enabling new business models. Our semantic solution makes it possible to exchange information with a commonly agreed meaning and format. In addition, our approach enables easy integration of other digital twins to be coupled and connected in the future, driving innovation. 
 
The CODIT solution is currently being used in a demonstration greenhouse with tomato plants during the growing season from October 2023 until October 2024. It generates weekly multiple greenhouse climate strategies and projects the resulting production and energy costs over the entire period. The crop manager can select the best strategy and receives the settings for the climate computer. Based on the feedback from the crop manager in our demonstration greenhouse, the CODIT software is being further finetuned. The proposed CODIT strategies are being compared with the actual plant development and energy consumption in the greenhouse. 
 
In this presentation, we will discuss the lessons learned in this use case when using a common ontology for coupling digital twins.",no,,,
112,5,sem24-industry,A linked data ecosystem for generating information products,Wouter Beek and Flores Bakker,2024-05-28 05:36,2024-05-28 05:39,,"information products
SHACL Advanced Features
report generating
documentation generation
budgeting",accept,no,no,"Initial situation
Large organizations must create a large quantity of information products. For example, the ministry of Finance must create a budget for government spending; a pension fund must create overviews of how much pension their customers have accrued; and municipalities must create building permits for project developers who want to construct new housing. Currently, most of the logic for constructing such information products is hidden in programming code that is difficult to understand, verify, and maintain.

Approach
We are able to create information products with linked data business rules, by utilizing the computational capabilities of SHACL Advanced Features. By doing so, we are able to express not only the data, but also computation over the data, as traceable and verifiable linked data. The resulting information products are represented in open standards like HTML, CSS, and SVG, that are compatible with the linked data standards.

Business value
The benefit of using linked data business rules for creating information products, is that the individual calculation rules are small and self-contained, while the complex execution of large rule collections is left to a standards-compliant SHACL engine. This allows organizations to create new information products, and maintain existing ones, with less maintenance overhead.

Prospects and recommendation
Our current ecosystem for creating information products with linked data includes vocabularies for HTML, DOM, XML, XPath, SVG, Mermaid, Manchester Syntax, and ReSpec. Our current applications are in generating budget tables, documentation, and formal reports in government administration services. We believe that this ecosystem will grow over time, and that an increasing number of information products will be created with this approach in the future.

The Git repositories for this ecosystem can be found on: https://github.com/floresbakker",no,,,
114,5,sem24-industry,Querying and Ingesting Linked Open Data about the Decisionmaking Process of the Flemish Government through Themis,Tom De Nies and Alvin Demeyer,2024-05-28 08:03,1970-01-01 00:00,,"decision-making
governmental data
linked open data
data catalogs
SPARQL",accept,no,no,"In recent years, Flanders has been investing heavily in the openness of government. 
Until 2019, decisions of the Flemish government were published in purely human-interpretable form, on a Drupal website coupled to their document management system. However, the lack of semantics made it challenging for applications to consume this data.

For this purpose, redpencil has developed the Themis open data portal (https://themis.vlaanderen.be/), where information regarding the decision-making process of the Flemish Government is disseminated in the form of Linked Open Data. More specifically, every non-confidential decision that is made during the weekly council of ministers in Flanders is automatically exported from the government's platform and published to Themis in the form of RDF. To maximize interoperability, the data is structured according to the OSLO data model, an ontology created specifically for this purpose with the cooperation of all parties involved with the decision-making process.

While the intention of Themis is to stimulate reuse in semantic applications, the data is accessible for a wide range of consumers. The main consumer of this data today is the governments own press release website which provides human-readable information about the decisions made by the government.

There are two main ways to access the data (other than through the browser). On the one hand, it can be ingested through DCAT data catalogs, which is useful for applications that want all the data, all at once, for example to analyze it in bulk. On the other hand, the data can be queried using SPARQL, which is useful for applications that are interested in specific data, about specific decisions or topics. I will briefly introduce the technicalities behind both methods, and provide real-world examples of where this approach is used. To conclude, I will showcase a number of useful scenarios and third-party integrations that exist today. ",no,,,
115,5,sem24-industry,Knowledge Graph Matching for deriving Recommendations of Digital Agricultural Technologies to Farmers,"Daan Di Scala, David de Best and Jack Verhoosel",2024-05-28 08:08,1970-01-01 00:00,,"Common Semantic Model
Ontology
Knowledge Graph
Recommender System",accept,no,no,"Digital technologies as solutions within the agricultural sector are rising, but introducing the most suitable Digital Agricultural Technologies (DATs) to farmers remains difficult. Within the QuantiFarm project, we aim to match farmers with DATs from different sources and provide them with insights into the DATs. For this, we have developed the QuantiFarm Semantic Model (QCSM), an ontology including relevant agricultural concepts that allows for data standardization. We ingest data from three different sources into the QCSM based knowledge graph, hosted in a triple store according to the RDF standard. We then deploy a recommender tool to match farmers with relevant DATs. Through a personalized interface, a user of the QuantiFarm Search & Recommender Tool can search, filter and receive recommendations which are relevant to their Farmer Profile. The recommendations are based on mappings between the farmers’ and the DATs’ attributes in the QCSM knowledge graph. 

The benefits of the semantic approach include the transparency and interpretability of the reasoning behind the recommendations, as well as the harmonization of data from multiple sources. However, dealing with outlying and contradicting data remains a challenge, as well as ensuring correctness and stability of both the triple store and search & recommender tool. Future steps include improving on the matching methods, by using alternative knowledge graph based recommender methods such as path based linking, which will require the necessary updates to the QCSM. We can provide a demonstration during the SEMANTiCS conference, as the current version of the Search & Recommender Tool is available online as part of the QuantiFarm toolkit (https://quantifarmtoolkit.eu/).",no,,,
116,5,sem24-industry,Leveraging Business Q&A with LLMs over Product Knowledge Graphs,Amir Laadhar and Nikhil Acharya,2024-05-28 11:04,2024-06-10 08:32,,"Knowledge Graphs
Large Language Models
Question Answering
Product Information Management Systems (PIM)",accept,no,no,"In the product information management (PIM) systems domain, we face the challenge of not only managing diverse data but also making it contextually relevant to business needs. Traditional PIM systems often fall short because they cannot provide meaningful contextual knowledge about product information within the same company. 
We enhance PIM systems with knowledge graphs, significantly improving decision-making by integrating all data sources into a single source of truth. The knowledge graph uses different ontologies and taxonomies to structure and integrate data from various sources, including internal and external product information. 
By integrating generative large language models (LLMs) with knowledge graphs, we enable accurate natural language question answering and specific product retrieval. A simple RAG (Retrieval-Augmented Generation) architecture usually fails in question-answering tasks over a knowledge graph. Therefore, we introduce a Graph RAG approach for PIM systems question-answering. 
We present PIM systems powered by knowledge graph and the Graph RAG approach, which highlights the system's capability to answer business questions using natural language. The integration of LLMs with PIM systems knowledge graphs results in context-aware answers to complex business questions in natural language using Graph RAG or query languages using SPARQL. It empowers businesses with comprehensive insights, driving informed decision-making in the dynamic product landscape.",no,,,
117,5,sem24-industry,From UML to OWL: Enterprise Architect as an ontology editor,"Wouter Beek, Linda van den Brink and Elena Slavco",2024-05-28 11:53,1970-01-01 00:00,,"UML
OWL
ontology editor
Enterprise Architect
data model",accept,no,no,"Initial situation
Many companies who adopt linked data technologies for the first time, struggle with the creation of ontologies. Most data architects are used to modeling paradigms like UML, and in applications like Enterprise Architect or PlantUML. As a result, linked data projects often experience stagnation at the start, because there is too little experience with ontology development, and creating an ontology from scratch takes a lot of time.

Approach
We introduce a new approach, where existing data models are automatically transformed into ontologies. We build on existing SPARQL-based approaches that transform UML models (in XML or via an ODBC connection) to the Meta Information Model (MIM) standard. Subsequently, MIM can be transformed into OWL+SHACL. Existing approaches require that the source UML model is specifically annotated for use with the MIM standard, but data architects in companies often do not want to add such annotations. We therefore also automate the addition of MIM annotations. This results in a completely automated pipeline from UML tools like Enterprise Architect to an OWL+SHACL model.

Business value and benefits
By being able to generate the ontology from an existing data model in a fully automated way, companies can get their linked data projects underway much quicker. This allows companies to focus on other aspect, like value creation and data quality earlier in the project, which results in a better ROI from the project.

Prospects and recommendation
We believe that the best tool for modeling OWL+SHACL is sometimes a widely adopted UML tool. This is specifically the case when there is a lack of experience with semantic modeling. We think that our approach will be used in a growing number of commercial project, and will allow linked data projects to be more successful.",no,,,
118,5,sem24-industry,Cortex: An Experimentation for a e-Health Data Hub,Pauline Armary and Brice Sommacal,2024-05-28 11:53,1970-01-01 00:00,,"Medical Ontology
Interoperability
Data Access Management
e-Health",accept,no,no,"The Cortex project initiated by the Service de Santé des Armées (French Health Services for military forces) aims to bring together all the data available within the different services and spread across many information systems. The solution proposed was the construction of a knowledge graph based on the ontological modeling of the medical domain to create a shared vocabulary across the different services with integration of international standard for the medical terminology. The use of a semantic interface between the different data systems and the user querying the data provides a unique end-point for all information system in a transparent manner. The system also integrates an access management system which differentiate the kind of information accessible depending on the profile of the user. In future works, the project will extend to integrate data from the hospital information system to offer more complete epidemiological analysis.",no,,,
122,5,sem24-industry,Leveraging Knowledge Graphs to prevent fraudulent payments for the Swedish Government,Joakim Nilsson,2024-05-28 13:53,2024-06-28 09:09,,"Fraud detection
Knowledge Graphs
Neo4j
Labeled Property Graphs",accept,no,no,"Leveraging Knowledge Graphs to prevent fraudulent payments for the Swedish Government
-	A talk by Joakim Nilsson, Knowledge Graph Lead at Capgemini Sweden


Since 2023, Capgemini has been assisting the Swedish Government in addressing fraudulent payments through the implementation of Knowledge Graph technology. This project employs a Labelled Property Graph approach using Neo4j as the graph database. 
Graph analytics, empowered by Graph Data Science, excels in fraud detection by identifying and exposing connections within fraudulent networks, providing investigators with crucial insights into their operations. Despite fraudsters efforts to conceal their identities and locations, graph analytics can detect fraudulent activities effectively. Its intuitive visual representation and ability to reveal hidden relationships make It a superior tool for uncovering and understanding the complexities of fraud.

When used for fraud detection, a graph data model links individuals, transactions and organizations to uncover patterns and indicators of fraud by comparing the connections with known fraud patterns in order to find similarities or anomalies in the data. 
In the end saving the Swedish taxpayers money and preventing fraudulent behavior.",yes,,,
124,5,sem24-industry,Multilingual linguistic word sense disambiguation for semantic annotations,"Robert David, Anna Kernerman, Ilan Kernerman and Assaf Siani",2024-05-28 19:07,1970-01-01 00:00,,"semantic annotations
linguistic
lexicographic
multilingual
large language models
word sense disambiguation",accept,no,no,"Knowledge models, constructed from vocabularies and ontologies, establish a formal basis to enable semantic annotations, which can then support use cases like semantic search and recommendations. In such a scenario, we face the challenges of word sense disambiguation (WSD), multiword expressions (MWE), and multilinguality (of models and content).

- For WSD and MWE, we need to exploit additional contextual knowledge to differentiate the word senses and identify expressions in the content.
- For multilinguality, we aim to support content that comes in a mix of languages, as well as querying across languages with an input query having a different language than the matching content.

To support both goals, we propose a combination of knowledge models, multilingual linguistic data (including lexicographic resources) and large (or massively multilingual) language models (LLMs). Via dictionary crossovers and other lexical information on multiple languages, we implement cross-language queries, and with the integration of LLMs we use these quality language resources to drive the disambiguation for the semantic annotations, thereby implementing a hybrid LLM-augmented Semantic AI system.

We will present the research project carried out jointly by Semantic Web Company and Lexicala by K Dictionaries, including our approach and methodologies along with the preliminary results of our experiments on converging language resources, knowledge graphs, and large language models.
",no,,,
126,5,sem24-industry,How Shared Knowledge Graphs Help Us Build Distributed Applications Within The semantic.works Framework.,"Aad Versteden, Niels Vandekeybus and Felix Ruiz de Arcaute",2024-05-28 20:26,2024-05-28 20:37,,"rdf
linked data
knowledge graph
interoperability
data sharing
semantic works",accept,no,no,"In this talk, we will discuss how shared (semantic) knowledge graphs help us not only build applications but also distributed applications. We will begin by defining our understanding of knowledge graphs, briefly introducing RDF and how it can lead to a common understanding for both machines and humans.

Then, we will explore how these concepts can be used as a foundation in ""Semantic.works"" (a lightweight microservice framework), thereby vastly facilitating code reuse and feature plugability. Next, we will illustrate how semantic knowledge graphs enable the reuse of data across multiple applications, even for data created by completely independent parties.

We will conclude with a thorough presentation of a production use case where knowledge graphs are shared and extended live across multiple applications. (Live demo if possible, else we'll show a pre-recorded demo with live comments.)

During the talk, we want to highlight the challenges and points of attention required for successful sharing of knowledge graphs. Our main takeaway is that you shouldn't be afraid of treating RDF as a first-class citizen in your application. It's production-ready technology (as proven by semantic.works) with a huge potential benefit in breaking up data silos, and being one step closer to a distributed, interoperable internet as initially envisioned by Tim Berners-Lee.",no,,,
127,5,sem24-industry,Next-Generation Cybersecurity: Integrating Knowledge Graphs and Neuro-symbolic AI with STIX and TAXII,Jans Aasman,2024-05-28 20:41,1970-01-01 00:00,,"Neuro-symbolic AI
Knowledge Graph
Graph RAG
Cyber Security
Cyber Threat Intelligence
Threat Detection
Machine Learning
Symbolic Reasoning
Neural Network",accept,no,no,"In the ever-evolving landscape of cybersecurity, the integration of Knowledge Graphs and Neuro-symbolic AI presents a leap forward in enhancing threat detection and response capabilities. This presentation explores the synergistic combination of these technologies, particularly within the framework of Structured Threat Information Expression (STIX) and Trusted Automated Exchange of Intelligence Information (TAXII).

Knowledge Graphs, which represent data in a structured, interconnected format, offer a powerful means to capture the complex relationships inherent in cyber threat intelligence (CTI). By leveraging Knowledge Graphs, organizations can create a comprehensive and dynamic representation of cyber threats, enabling more effective correlation and contextualization of disparate data points. This holistic view is crucial for identifying sophisticated attack patterns that may be missed by traditional, isolated analysis methods.

Neuro-symbolic AI, which combines the strengths of neural networks and symbolic reasoning, further enhances the capabilities of Knowledge Graphs. Neural networks excel at detecting patterns in large datasets, while symbolic reasoning provides a mechanism for understanding and manipulating complex concepts and relationships. This combination allows for more robust and accurate threat detection, as it can leverage both data-driven insights and human-understandable rules.

STIX(1)1, as a standardized language and serialization format for CTI, plays a pivotal role in this ecosystem. It enables organizations to share threat information in a consistent, machine-readable manner, facilitating collaborative threat analysis and automated detection and response. The integration of STIX with Knowledge Graphs and Neuro-symbolic AI enhances the ability to process and interpret CTI, leading to faster and more effective responses to cyber threats.

TAXII(1), the protocol designed for exchanging CTI over HTTPS, complements STIX by providing a scalable and standardized method for information sharing. By defining an API that aligns with common sharing models, TAXII ensures that the exchanged CTI can be seamlessly integrated into existing security workflows and systems. This interoperability is crucial for fostering a collaborative defense against cyber threats.

This presentation will delve into the practical applications of Knowledge Graphs and Neuro-symbolic AI in cybersecurity, highlighting their potential to transform threat detection and response. In particular we will be focusing on how to automatically extract structured STIX objects from unstructured incident reports. By incorporating the principles of STIX and TAXII, we demonstrate how these technologies can enhance collaborative threat analysis, automated threat exchange, and ultimately, the overall security posture of organizations.

(1)https://oasis-open.github.io/cti-documentation/
",no,,,
129,5,sem24-industry,A Semantic Layer for Data Spaces: how interlinked vocabularies provide interoperability inside and between Data Spaces,Robert David and Martin Kaltenböck,2024-05-29 08:51,2024-06-07 15:54,,"data spaces
semantic layer
vocabularies
metadata mapping
semantic annotation
data consolidation
semantic interoperability",accept,no,no,"Data Spaces have become a big movement in Europe and beyond. The idea of trusted and secure data sharing between organisations - by keeping full data sovereignty - supports the approach of establishing the European digital single market and the development of new business models and it fosters value generation by making more use of the available data. With more and more Data Spaces and data available, the question of semantic interoperability becomes crucial and needs action. So how can we ensure that inside a Data Space AND between different Data Spaces relevant data can (i) be easily identified and then (ii) be easily and cost-efficiently used and integrated. 

To solve this we suggest a “Semantic Layer for Data Spaces”: an ecosystem of trusted, referenceable and interlinked controlled vocabularies accompanied with a set of related services, that allow to: (i) analyse metadata and also data (as far as available/accessible) inside of a Data Space and afterwards (ii) semantically annotates such datasets, thereby enriching the metadata and data. In addition, the Semantic Layer services are of value to (iii) support automated metadata mapping during metadata ingestion, and (iv) provide a service for semantic harmonisation of data. Together, these envisioned services form an implementation for augmented data catalogs which supports automated and intelligent metadata management and semantic interoperability based on Semantic AI.

The Semantic Layer for Data Spaces can be established as (i) a centralised service for Data Spaces that follow current standards or (ii) as federated service per Data Space to enable more flexibility, whereby we recommend the centralised approach. The implementation approach for this Semantic Layer is aligned with and even expands the IDS Reference Architecture Model Vocabulary Hub component, and supports the re-use of existing domain specific controlled vocabularies. Thereby this approach supports the recommendations of the High Level Forum of European Standardisation on data interoperability (https://ec.europa.eu/docsroom/documents/58914). The authors will present this approach along real world examples from the 2 EU funded projects: DataBri-X (https://databri-x.eu/), Underpin (https://underpinproject.eu/) and the Austrian-German lighthouse project: https://www.champi40ns.eu/.",no,,,
133,5,sem24-industry,Simplifying and Automating Product Compatibility with Rules-based AI aka “What goes with what?” ,"Peter Crocker, Thomas Vout and Philip Foster",2024-05-29 17:12,1970-01-01 00:00,,"Knowledge Graph
Semantic Reasoning
Semantic Web
Data Enrichment
LLM
Generative AI
Datalog
SPARQL
OWL
RAG
Retrieval Augmented Generation
Database
Configuration Management
Compatability Management
Product Configuration
Manufacturing
Retail
Use Case",accept,no,no,"In today's complex world, creating comprehensive solutions from compatible products presents significant challenges across various sectors. From manufacturing to retail, the overwhelming number of possible component combinations increases exponentially with scale, complicating the decision-making process for customers seeking tailored solutions. This complexity often leads to slow and expensive outcomes when trying to automate the calculation of optimal combinations. 

 

Throughout this demo we’ll follow two real-world applications of knowledge graphs and semantic reasoning: finding a range of furniture and household items that complement one another following the work of a global furniture retailer; and designing custom electro-mechanical systems from thousands of parts in order to meet specific and complex high-level requirements for the world's largest manufacturer of factory automation equipment. Although this is where we will focus, we’ll also highlight parallel use cases in a variety of industries that could massively benefit from this technology. 

 

Traditional search algorithms currently in use tend to be based on crowd behaviour, and fail to efficiently navigate through data that lacks explicit concepts, resulting in poor recommendations and a frustrating user experience. For example, a customer looking for a furniture set on an online retail site might struggle to find options that meet all their criteria like size, colour, style, material, and budget, along with practical considerations such as ease of cleaning for homes with pets or being safe for children. 

 

The solution lies in semantic reasoning, otherwise known as rules-based Artificial Intelligence (AI). By adding expert knowledge to the system in the form of rules, this technology contextualises and analyzes the relationships between products, akin to human reasoning. This AI-driven approach scales expert knowledge, alleviating the burden of manually configuring every potential product combination, while also introducing a method to automatically update the product catalogue—incremental reasoning. Incremental reasoning ensures the knowledge base is always consistent and up to date with the latest changes, calculating new configurations as new products are added and removing old when the relevant products are discontinued.  

 

Our presentation will demonstrate how this innovative use of knowledge graphs and rules-based AI significantly enhances customer experiences and reduces operational costs, regardless of the industry. ",no,,,
134,5,sem24-industry,How Semantic Technology Brings Clinical Knowledge to Decision Support in an Instant ,"Peter Crocker, Thomas Vout and Philip Foster",2024-05-29 17:14,1970-01-01 00:00,,"Knowledge Graph
Semantic Reasoning
Semantic Web
Data Enrichment
LLM
Generative AI
Datalog
SPARQL
OWL
RAG
Retrieval Augmented Generation
Database
Pharma Tech
Clinical Tech
Medical Tech
Healthcare
Digital Assistant
On-device",accept,no,no,"Semantic technology use cases for medical and pharma applications involve some of the largest and most complex ontologies and knowledge graphs. In this presentation we will hear about the first intelligent assistant of its kind that relies on a hybrid AI (logical semantic reasoning and ML image recognition), fully integrated on-device in ultrasound platforms—showcasing the SUOG (Smart Ultrasound in Obstetrics and Gynecology) project1. 

 

The use of semantic technology in this project will be explored along with how performance and scalability was achieved on-device by using an in-memory knowledge graph and reasoning engine. Examples and parallels will be highlighted showing this technology’s relevance with other medical applications and the pharma industry. 

 

A demo of the technology will give a closer look at how reasoning works in the context of knowledge-based AI applications. The combination of both machine learning and rules-based AI (reasoning) creates an incredibly powerful solution that serves to improve accuracy and enrich data, enhancing the ability of machine learning, here image recognition, to produce instant clinical knowledge and decision support on a hand-held device. 

 

We make reference to the use of LLMs in a clinical setting, pros and cons, and how they could work alongside expert knowledge where lives may literally be at stake. 

 

Finally, practical tips on how and where to get started with reasoning will be shared. Audience members will come away able to take their first steps towards implementing this technology in their own applications with examples for inspiration that follow our clients, including a healthcare device manufacturer. ",no,,,
135,3,sem24-pd,Toward Exploring Knowledge Graphs with LLMs,"Guangyuan Piao, Mike Mountantonakis, Panagiotis Papadakos, Pournima Sonawane and Aidan Omahony",2024-05-30 09:26,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/parklize/LLM4SPARQL

The source code and prompt templates used for the example implementation
will be made publicly available on the URL once accepted.
(Data Availability) https://github.com/parklize/LLM4SPARQL

The source code and prompt templates used for the example implementation
will be made publicly available on the URL once accepted.
(Ontology Availability) 
(Demo link) ","SPARQL
Large Language Models
Question Answering
Knowledge Graphs",accept,yes,yes,"Interacting with knowledge graphs (KGs) is challenging for non-technical users with information needs who are unfamiliar with SPARQL and the underlying KG schema. Previous KG question answering systems require ground-truth pairs of questions and SPARQL queries or fine tuning (Large) Language Models (LLMs) for a specific KG, which is time-consuming and demands deep expertise. In this poster, we propose a framework for exploring KGs by generating SPARQL queries using LLMs solely in response
to given questions in a zero-shot setting for non-technical end users, without the need for ground-truth pairs of questions and queries or fine-tuning LLMs. We present an example implementation based on the framework and share preliminary experimental results, indicating that exploring a KG using LLM-generated SPARQL queries with reasonable complexity is possible in such a challenging setting.",no,,,
136,5,sem24-industry,From Law to data and back - implementing and customizing a method,L van Bergen and Nicole Groot-Nibbelink,2024-05-31 21:30,2024-06-10 09:08,,"Belastingdienst
Information law
Tax law
Knowledge management
Modeling with lineage
Method implementation
Enterprise Knowledge Graph
Semantic Interoperability",accept,no,no,"An insight into how the Tax Authorities of the Netherlands uses semantic knowledge models. Lawyers and IT professionals often do not speak the same language, but they must still understand each other well. Knowledge and meaning from legislation and regulations needs to be described in such a way that it can be used in implementation. Think about models of concepts, definitions, object types with properties, data and rules, all linked together. We developed a method to make models and we publish our models in a model library (RDF, platform and tool independed). These models are used as input for the development of software. Our story is not about software. Software is about automation. Our story is about what precedes it.",no,,,
138,3,sem24-pd,Facilitating Learning Analytics in Histology Courses with Knowledge Graphs,"Jimmy Walraff, Adreas Coco, Guillaume Delporte, Merlin Michel, Allyson Fries, Valérie Defaweux and Christophe Debruyne",2024-06-05 10:52,2024-08-15 16:03,"(Student) Yes
(Publication Resources) Yes
(Code Availability) 
(Data Availability) https://github.com/chrdebru/papers/2024-09-semantics/ontology.owl 
(Ontology Availability) 
(Demo link) ","KG Construction
Learning Analytics
Ontology Engineering",accept,yes,yes,"We report on an ongoing learning analytics project at the University of Liège, in which we want to analyze student interactions on Cytomine for a histology course. Cytomine provides tools for medical image annotation and an API that has been used for learning analytics. The problem, however, is that the data obtained from Cytomine has implicit semantics and requires many data preprocessing and integration steps. This poster presents the prototype KG we have built to address these problems. The KG adopts PROV-O to distinguish activities from their outcomes, addressing some of the issues faced in the past. We also demonstrate that the KG can be used in Jupyter notebooks, though learning analytics is left for future work. It did demonstrate that the data analysis process has become more declarative and transparent, as data is analyzed starting from SPARQL queries. We focused on one project in Cytomine, and future work consists of integrating additional projects. We also plan to investigate the development of more self-contained KG generation techniques as we have no direct access to the Cytomine application.",no,,,
141,5,sem24-industry,Improving term networks through the detection of semantic perspectives,"Maya Sappelli, Hans Fugers, Bart Kleijngeld and Marijn Siebel",2024-06-07 10:21,1970-01-01 00:00,,"knowledge graphs
text mining
semantic perspectives",accept,no,no,"We present an industry use case in which we discuss the practical use of a formal list of definitions. The list of definitions will be used as a basis for a knowledge graph that will serve knowledge panels when interacting with information in the company and that will help improve question answering using large language models. We evaluate whether the list is complete, whether the terms are relevant and whether there might be ambiguity in these terms. For this purpose we have analyzed a large body of company documents that represent how employees use these terms in practice, as well as a body of formal documents that represent how these terms are used in the industry in general. This has lead to the finding that some of the terms may not be ambiguous, but can be interpreted differently. There is a constant balance in a term list between completeness, level of abstraction and relevance. Determining which terms may lead to confusion because of multiple interpretations is a relevant step forward in creating usable knowledge graphs. ",no,,,
142,5,sem24-industry,Implementing a Data Fabric for the Water Authority of Limburg,Elvin Dechesne,2024-06-07 14:03,1970-01-01 00:00,,"Topbraid EDG
MongoDB
Data Fabric
Knowledge Graphs
Semantics
Water Authority of Limburg
Digital Twin
Information Modelling
Integrations
Multimodel",accept,no,no,"The Dutch Water Organization Limburg (WL) has recognized the critical need to become more data-driven due to the increasing risks posed by climate change. The ability to make informed decisions based on high-quality, up-to-date information is essential. WL identified issues such as poor data quality, lack of asset status awareness, and underperforming IT resources during crises, which need to be addressed.
Our project, the 'Information Hub' or 'Informatieknooppunt' (IKP), integrates TopQuadrant's Enterprise Data Governance (EDG) and MongoDB within a Microsoft Azure infrastructure and Azure DevOps environment to form a comprehensive data fabric. This multimodal solution supports semantic knowledge graphs, which are vital for managing diverse data sources and providing a cohesive view of the operational landscape.
The IKP leverages information modeling and data governance based on the 'Metamodel voor Informatie Modellen' (MIM), enabling the organization to become 'information-aware'. By integrating enterprise architecture and business analytics with the IKP knowledge graph, the organization benefits from shared and newly acquired knowledge. The solution facilitates collaboration among business users, data scientists, and developers, enhancing the creation of information products like dashboards, analytics reports, and spatial awareness solutions.
Key benefits include the implementation of a hybrid data/DevOps methodology, availability of reference and master data as Linked Data, and the use of knowledge graphs to add value from legacy on-premise data sources. The semantic technology employed adheres to the FAIR principles, promoting a data-driven culture in line with the Common Ground vision for easy data exchange and access. Additionally, the IKP offers a pathway to understanding data governance and technical lineage across various data sources, providing a foundation for integrating Large Language Models and Generative AI.
Towards the production phase, new domains are continually incorporated, enriching the knowledge graphs' context and depth. It is essential to balance long-term goals with quick wins to demonstrate the solution's direction and generated value. Ultimately, adopting semantic technology and information modeling is crucial for driving cultural change towards data-awareness.",no,,,
143,3,sem24-pd,EA-to-RDF: Pain-Free integration of RDF and UML,Bob Janssen and Ghislain Atemezing,2024-06-07 16:56,2024-06-07 16:56,"(Student) 
(Publication Resources) N/A
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) ","UML diagrams
RDF
OWL
Ontology
Enterprise Architect
Model Based Systems Engineering",accept,yes,yes,This article presents an addon to the popular Enterprise Architect (EA) tool that lets modellers import existing ontologies into EA as classes and export class models to ontologies. The addon implements the SEMIC guidelines that standardise the UML-RDF mapping. The tool has been tested against some large UML diagrams from the railway domain and the generated outputs (RDF and UML) demonstrate the usefulness of EA-RDF both from UML experts and ontology developers. This approach bridges the gap between ontologies and IT engineering in UML; modellers at ease in UML but unfamiliar with triples can now easily access existing ontologies and publish their models as triples.,no,,,
148,5,sem24-industry,Smart Search: How content enrichment and knowledge graphs can help ground your LLM to improve search experience,"Harold Selman, Paul Verhaar and Pauline van Nies",2024-06-10 15:30,1970-01-01 00:00,,"Semantic Search
Modular RAG
Large Language Models
Knowledge Graphs
Content Enrichment
RAG
NLP",accept,no,no,"The evolution towards smart, semantic search capabilities is pivotal in search technologies. This talk delves into a modular, composable solution at this transformation's forefront. Traditional search systems often fall short of understanding the context and semantics of user queries, leading to suboptimal results. Our proposed modular RAG setup solution addresses these shortcomings by offering unparalleled flexibility and customization. “I want our website to be the place where users find answers on their questions and Smart Search is helping us do that”. We showcase the success of this setup by showing how we made this website better searchable, comparing it to the old website and its user experience. Organizations can seamlessly integrate various language models without altering the core architecture, such as OpenAI API, Azure OpenAI API, or open-source LLMs from Huggingface. The solution’s validation process employs benchmark datasets and user-specific data, ensuring the chosen configuration meets the desired search objectives. Deployable on any Kubernetes cluster, it guarantees data privacy and security, whether in a private or public cloud environment. The incorporation of knowledge graphs and content enrichment techniques grounds large language models and enrich search queries, particularly with non-textual, unstructured data. This talk advocates adopting such a semantic solution, highlighting its business value and benefits to the search domain.",no,,,
149,5,sem24-industry,Leveraging Knowledge Graphs and Machine Learning for Automated CO2 Footprint Calculation of Buildings,Lokesh Sharma and Martin Voigt,2024-06-10 20:07,1970-01-01 00:00,,"Knowledge Graphs
SHACL
RDF
Machine Learning
Semantic Web",accept,no,no,"The building design, construction, and operation sectors face sustainability challenges due to a fragmented manual processes, lack of integrated, automated solutions. There is a need for: 1) a scalable solution that complies with the Environmental, Social, and Governance (ESG) factors to identify material risks and growth opportunities 2) a comprehensive solution that can effectively extract the semantic information from both structured and unstructured data and further integrate this knowledge across various data silos for downstream applications. However, this presents some challenges including but not limited to, ambiguous and interconnected domain concepts, necessity for domain- and customer-specific business knowledge, large volumes of (un)structured data, etc.. Our cloud-based semantic data modeling approach ai:cm can effectively address these challenges using advanced information retrieval techniques and semantic data modeling. The extracted knowledge is structured into a knowledge graph (RDF triples) and automatically linked to information sources like material databases. This could serve as the foundation for CO2 footprint calculations in industries such as construction, building design, and operations. The success of such an implementation can be measured by various KPIs, primarily the degree of automation, quality of attribute extraction, and accuracy of recommendations. The semantic aspect of our solution ai:cm offers reusability and interoperability of existing data models, like Industry Foundation Class (IFC), extensibility for new data inputs and requirements, validation and execution of business rules using SHACL shapes, and UI generation for ad-hoc updates. ",no,,,
151,5,sem24-industry,Teaming.AI: Enabling Dynamic Knowledge Graph Representations in Process-Driven Application Domains,"Franz Krause, Heiko Paulheim and Bernhard Moser",2024-06-11 10:39,1970-01-01 00:00,,"Knowledge Graph
Process Modeling
Industry 5.0
Smart Manufacturing
Semantic Web",accept,no,no,"The Teaming.AI project, funded through the European Commission's Horizon 2020 Research Programme under Grant Agreement Number 957402, intends to provide a human-AI teaming platform for maintaining and evolving AI systems in manufacturing. The project started back in Initiated in January 2021 and concluded in June 2024, a total of 15 research and industry partners from across Europe collaboratively developed a novel operational framework to cope with the heterogeneity of data types, the uncertainty of decisions, and the dynamic changes in the context of human-AI collaboration. A key component of the project is the Teaming.AI knowledge graph (KG), which acts as a digital shadow of human-AI collaboration scenarios. This KG has been deployed in three real-world manufacturing use-cases aimed at enhancing efficiency in plastic molding (Spain, Turkey) and improving the ergonomic health of workers in large part manufacturing (Spain). The concept behind the Teaming.AI KG introduces significant challenges such as the conceptualization of the application in an ontology and the dynamic changes in the context of human-AI collaborations, with updates occurring in seconds rather than hours. To counteract these challenges, the Teaming.AI KG is built upon a modular KG population approach based on automated KG transformations of process model executions. Specifically, task expertise is treated as sub-domains, facilitating the creation of domain-specific ontology definitions. Additionally, the KG entities that represent the executions of process elements, such as tasks, events, or transitions, can be easily enriched with task expertise. This enrichment enables an automated semantic linking between sub-domains through the process executions, enhancing the KG's functionality and responsiveness. This industry talk features an interactive demonstration that showcases the implementation of the dynamic KG generation and population approach within one of the real-world human-AI collaboration scenarios.",no,,,
153,3,sem24-pd,"8-star Linked Open Data Model: Extending the 5-star Model for Better Reuse, Quality, and Trust of Data",Eero Hyvönen and Jouni Tuominen,2024-06-14 14:14,2024-08-15 09:51,"(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) Various datasets are available CC BY 4.0 at:

https://ldf.fi
(Ontology Availability) Various applications on top of the LDF.fi platform are available CC BY 4.0:

https://github.com/SemanticComputing
(Demo link) https://ldf.fi","Linked open data
Semantic web
5-star open data
Publishing model",accept,yes,yes,"This paper argues, based on lessons learned in developing several in-use Linked Open Data (LOD) services and applications, that biggest challenges of re-using LOD are missing data schemas, quality of data, and trust on the correctness of data with respect to the real world. To encourage data publishers to address the issues, three more stars are proposed for the classical 5-star model coined by Tim Berners-Lee. The proposed model is supported by the Linked Data Finland platform, a living lab environment in use supporting LOD publication for data-driven research and application development.",no,,,
154,3,sem24-pd,Continuous Knowledge Graph Quality Assessment through Comparison using ABECTO,Jan Martin Keil,2024-06-18 13:48,2024-06-18 13:49,"(Student) 
(Publication Resources) Yes
(Code Availability) https://doi.org/10.5281/zenodo.11522479
(Data Availability) https://doi.org/10.5281/zenodo.7843823 ; Wikidata and DBpedia Space Travel Data Comparison with ABECTO
https://doi.org/10.5281/zenodo.7843835 ; Units of Measurement Data Comparison with ABECTO
(Ontology Availability) http://w3id.org/abecto/vocabulary ; ABECTO Vocabulary, ontology for pipeline and result description
(Demo link) https://github.com/fusion-jena/abecto-ci-demo ; ABECTO CI demo code and instructions
https://github.com/fusion-jena/abecto-ci-demo-example ; ABECTO CI demo example results","Continuous Integration
Knowledge Graph Engineering
Knowledge Graph Quality
Ontology Engineering
Ontology Quality",accept,yes,yes,"Maintaining accuracy and completeness of RDF knowledge graphs is an important but challenging task. As constraints checking can only spot outliers, knowledge graphs would need to be checked against reference data to obtain a reliable assessment. But this is often impossible due to the lack of suitable reference data. An alternative is the comparison with other, overlapping knowledge graphs that possibly contain incorrect data, too. This can spot potentially incorrect values in the maintained knowledge graph. As knowledge graphs might evolve over time, this comparison must be done regularly. However, the regular comparison of data is an exhausting and error prone task, if done manually.

We present ABECTO, a command line tool for the automatic comparison of multiple possibly incorrect RDF knowledge graphs to monitor their accuracy and completeness. In our demonstration, we will showcase its application in a Continuous Integration scenario for the regular automatic check of the quality of a maintained knowledge graph.",no,,,
155,3,sem24-pd,Populating CSV Files from Unstructured Text with LLMs for KG Generation with RML,"Jan Maushagen, Sara Sepehri, Audrey Sanctorum, Tamara Vanhaecke, Olga De Troyer and Christophe Debruyne",2024-06-19 07:07,2024-08-18 17:54,"(Student) 
(Publication Resources) N/A
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) ","KG Construction
LLMs
End-user Involvement",accept,yes,yes,"We report on an exploratory study using Large Language Models (LLMs) to generate Comma-Separated Values (CSV) files, which are subsequently transformed into Resource Description Framework (RDF) using the RDF Mapping Language (RML). Prior studies have shown that LLMs sometimes have problems generating valid and well-formed RDF from unstructured texts, i.e., issues with RDF, not the contents. We wanted to test whether the generation of CSV led to fewer issues and whether this would be a viable option for allowing domain experts to be actively part of the Knowledge Graph (KG) population process by allowing them to use familiar tools. We have built a prototype illustrating this idea, and the results seem promising for further study. The initial prototype uses zero-shot training and is built on GPT-4. The prototype takes the unstructured text and the CSV file's structure as input and uses the latter to generate prompts to fill in the cells' values. Future work includes analyzing the effect of different prompting strategies. The limitation, however, is that such an approach only works for projects where domain experts work with spreadsheets for pre-existing mappings.",no,,,
156,3,sem24-pd,ORKG ASK: a Neuro-symbolic Scholarly Search and Exploration System,"Allard Oelen, Mohamad Yaser Jaradeh and Sören Auer",2024-06-21 09:01,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://gitlab.com/TIBHannover/orkg/orkg-ask/frontend
https://gitlab.com/TIBHannover/orkg/orkg-ask/backend
(Data Availability) 
(Ontology Availability) 
(Demo link) https://ask.orkg.org/","Neuro-symbolic AI
Large Language Models
Scholarly Knowledge Graphs
Scholarly Search System",accept,yes,yes,"Purpose:
Finding scholarly articles is a time-consuming and cumbersome activity, yet crucial for conducting science. Due to the growing number of scholarly articles, new scholarly search systems are needed to effectively assist researchers in finding relevant literature. 

Methodology:
We take a neuro-symbolic approach to scholarly search and exploration by leveraging state-of-the-art components, including semantic search, Large Language Models (LLMs), and Knowledge Graphs (KGs). The semantic search component composes a set of relevant articles. From this set of articles, information is extracted and presented to the user. 

Findings:
The presented system, called ORKG ASK (Assistant for Scientific Knowledge), provides a production-ready search and exploration system. Our preliminary evaluation indicates that our proposed approach is indeed suitable for the task of scholarly information retrieval. 

Value:
With ORKG ASK, we present a next-generation scholarly search and exploration system and make it available online. Additionally, the system components are open source with a permissive license. ",no,,,
157,3,sem24-pd,Towards Pattern-based Complex Ontology Matching using SPARQL and LLM,Ondřej Zamazal,2024-06-21 11:09,2024-06-24 08:33,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/OndrejZamazal/ComplexOntologyMatching-SEMANTiCS2024/blob/main/ComplexOntologyMatching_CAT_SEMANTiCS2024.ipynb
(Data Availability) 
(Ontology Availability) https://github.com/OndrejZamazal/ComplexOntologyMatching-SEMANTiCS2024

https://github.com/OndrejZamazal/ComplexOntologyMatching-SEMANTiCS2024/blob/main/experiment.md
(Demo link) ","Ontology
Ontology Matching
Complex Ontology Matching
Large Language Model
Knowledge Graph",accept,yes,yes,"Complex ontology matching is a process to match complex structures in ontologies. While many matching tools tackle simple ontology matching, complex ontology matching is still rare. However, one entity in one ontology can be similar to a complex structure (1-to-n) or even complex structures can be on both sides (m-to-n). Therefore, the application, e.g., data integration, must consider complex correspondences within ontology alignment. Our poster paper presents a pattern-based approach where particular SPARQL queries correspond to a specific pattern, e.g., Class by Attribute Type (CAT), for its detection. SPARQL queries are anchored to entities from simple correspondences on input. Complex correspondence candidates are verbalized to be validated by the Large Language Model (LLM). Further, we provide a zero-shot prompting preliminary experiment and evaluation. The poster paper is equipped with the Jupyter notebook for automation of the pipeline and the full report of the experiment at: https://github.com/OndrejZamazal/ComplexOntologyMatching-SEMANTiCS2024",no,,,
159,3,sem24-pd,A systematic approach towards higher quality linked open data at Nieuwe Instituut,Nora Abdelmageed and Lois Hutubessy,2024-06-24 10:24,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) https://collectiedata.hetnieuweinstituut.nl/the-other-interface/knowledge-graph
(Ontology Availability) 
(Demo link) ","Linked Open Data
Cultural Heritage
Data Quality
Entity Linking
Entity Resolution",accept,yes,yes,"Nieuwe Instituut (NI) houses the Dutch National Collection of Architecture and Urban Planning. This collection consists of about 4 million objects, including design drawings, 3D models, and photographs. As part of the program Disclosing Architecture, which is now in its sixth and final year, the Linked Open Data (LOD) project aims to share the richness of data within the collection with the public through semantic web technologies. This project will ultimately facilitate the exchange of cultural heritage data with related national and international institutions. Currently, Nieuwe Instituut (NI)’s collection management system contains inconsistent records due to changes in registration guidelines and the
migration from older collection management tools. Without documentation of these guidelines, it is impossible to establish consistent rules for the entire dataset. Yet, clean data is crucial for effectively showcasing NI’s collection to the public. In response, this paper introduces the Data Cleaning Initiative (DCI), a series of steps planned for the year 2024. The primary goal of this initiative is to clean and enrich the collection data at NI.",no,,,
161,3,sem24-pd,Data-Sovereign Enterprise Collaboration using the Solid Protocol,"Thorsten Kastner, Christoph Braun, Andreas Both, Dustin Yeboah, Sebastian Josef Schmid, Daniel Schraudner, Tobias Käfer and Andreas Harth",2024-06-24 15:53,2024-06-27 15:02,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/mandat-project
https://github.com/mandat-project/delegation-proxy
(Data Availability) 
(Ontology Availability) 
(Demo link) Screencast of demo: 
https://drive.google.com/file/d/1Be-fR0OdE2OCI0tcthXRjFjoKKGJyYAq/view?usp=sharing","Solid Protocol
Dataspaces
Data Value Chains
Zero Trust
Data Sovereignty",accept,yes,yes,"We demonstrate a system architecture for seamless and sovereign business-to-business (B2B) data sharing using the Solid Protocol. We highlight two core system components: The Rights Delegation Proxy allows organizations to internally manage and enforce access policies on requests from their employees to external data providers. The Data Provision Proxy allows organizations to share data from an external data provider in a privacy-preserving manner. Organizations define and enforce the policies for internal
and external data sharing themselves, thereby maintaining sovereignty in enterprise collaboration.",no,,,
163,3,sem24-pd,The Helmholtz Digitization Ontology: Representing Digital Assets in the Helmholtz Digital Ecosystem,"Said Fathalla, Gerrit Günther, Leon Steinmeier, Christine Lemster, Dorothee Kottmeier, Lakxmi Sivapatham, Pier Luigi Buttigieg, Volker Hofmann and Stefan Sandfeld",2024-06-25 15:25,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://codebase.helmholtz.cloud/hmc/hmc-public/hob/hdo
(Data Availability) 
(Ontology Availability) https://purls.helmholtz-metadaten.de/hob/hdo.owl ; hdo ontology
(Demo link) ","Metadata
Metadata Management
Data Management
FAIR
Harmonization
OWL
Bilingual ontology",accept,yes,yes,"The Helmholtz Association is actively digitizing research outcomes to drive progress and innovation. The large amounts of digital data and metadata are heterogeneous with respect to their formats as well as their semantic descriptions used for creation, storage, and exchange. A semantic frame of reference is required to facilitate interoperability throughout the Helmholtz digital ecosystem. 
This paper presents the Helmholtz Digitization Ontology (HDO), which is intended to serve exactly that purpose. HDO is a mid-level ontology that contains concepts representing digital assets relevant to the Helmholtz digital ecosystem, data creation, management, and exchange. HDO is developed within the framework of the Helmholtz Metadata Collaboration (HMC) with contributors from various scientific backgrounds. This leads to intrinsic harmonization of the output across these domains. Based on that, HDO serves as a harmonized and machine-actionable institutional reference that represents digital assets and procedures pertinent to their handling within Helmholtz.",no,,,
166,3,sem24-pd,PCFWebUI: Data-driven WebUI for holistic decarbonization based on PCF-Tracking,"Ajay Kumar, Marius Naumann, Kevin Henne and Mohamed Ahmed Sherif",2024-07-01 11:03,2024-07-05 16:11,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/dice-group/ClimateBowl-KGConverter
(Data Availability) 
(Ontology Availability) 
(Demo link) https://climatebowl.demo.dice-research.org","Knowledge Graphs
PCF Tracking
Energy Efficiency",accept,yes,yes,"The pursuit of corporate greenhoue gas neutrality has become increasingly critical due to heightened sustainability expectations, rising energy and CO2e costs, and stricter regulatory requirements. Key drivers, such as the mandated reduction of greenhouse gas emissions by 65% by 2030 compared to 1990 levels and the goal of achieving climate neutrality by 2045, necessitate immediate action toward decarbonization. In this paper, we introduce PcfWebUI, a data-driven tool developed to support companies in their decarbonization journey. PcfWebUI is built on real company data integrated into a knowledge graph, enabling efficient tracking and management of product carbon footprints (PCF), facilitating strategic planning and accurate monitoring to help companies meet stringent climate targets and progress toward climate neutrality. A demo of our system is publicly available at https://climatebowl.demo.dice-research.org/.",no,,,
168,3,sem24-pd,Linking application and semantic data with RDF Lens,"Arthur Vercruysse, Julian Rojas and Pieter Colpaert",2024-07-02 07:23,1970-01-01 00:00,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/ajuvercr/rdf-lens
(Data Availability) 
(Ontology Availability) 
(Demo link) ","RDF
SHACL
Functional programming
Lenses
JavaScript
Haskell",accept,yes,yes,"Linked Data is commonly regarded as an unfriendly data structure to be directly used by application developers. The (often unknown) triple-based structure of RDF graphs causes developers to struggle to extract the triples of interest and translate them into the object-like structure needed for their application. A generic, composable and reusable way to look into RDF graphs as plain objects would remove an important barrier for integrating Linked Data in all facets of application logic. We propose RDF Lens, a library based on ideas from the Haskell lens library that allows for composable and reusable data extraction units, called lenses. Value is shown by implementing a lens that generates a new lens based on a SHACL shape that extracts the semantic data into the desired plain object. Abstracting data extraction at the lens level allows for mixed extraction: using both custom extraction and declarative extraction, which could increase ease of use and reusability. The current implementation is a proof of concept that defines how to extract data from an RDF graph in JavaScript applications but does not allow (yet) writing or altering linked data with the same lenses. Future work would allow for creating and updating Linked Data in the same elegant way.",no,,,
169,3,sem24-pd,Classifying Scientific Topic Relationships with SciBERT,"Alessia Pisu, Livio Pompianu, Angelo Salatino, Francesco Osborne, Daniele Riboni, Enrico Motta and Diego Reforgiato Recupero",2024-07-02 14:00,1970-01-01 00:00,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/aleessiap/LeveragingLMforGeneratingOntologies
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Research Topics
Ontology Generation
Language Models
Knowledge Graph Generation
SciBERT",accept,yes,yes,"Current AI systems, including smart search engines and recommendation systems tools for streamlining literature reviews, and interactive question-answering platforms, are becoming indispensable for researchers to navigate and understand the vast landscape of scientific knowledge. Taxonomies and ontologies of research topics are key to this process, but manually creating them is costly and often leads to outdated results. This poster paper shows the use of SciBERT model to automatically generate research topic ontologies. Our model excels at identifying semantic relationships between research topics, outperforming traditional methods. This approach promises to streamline the creation of accurate and up-to-date ontologies, enhancing the effectiveness of AI tools for researchers.",no,,,
170,3,sem24-pd,Facilitating Search of the Virtual Record Treasury of Ireland Knowledge Graph using ChatGPT,"Alex Randles, Declan O'Sullivan, Lucy McKenna, Beyza Yaman, Lynn Kilgallon and Peter Crooks",2024-07-03 11:54,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/alex-randles/VRTI-ChatGPT
(Data Availability) 
(Ontology Availability) 
(Demo link) https://vrti-graph.adaptcentre.ie/gpt-search","KG Search
User Interface
ChatGPT",accept,yes,yes,"The Virtual Record Treasury of Ireland (VRTI) is an initiative to digitally recreate the contents of the Irish central archive which was destroyed during the Civil War. The project has created a Knowledge Graph (KG) to facilitate information discovery and reasoning over the recovered items.  However, complex queries must be created to retrieve data in the KG, which require a high level of technical expertise. In this paper, we explore the application of Large Language Models (LLMs) to facilitate searching of the VRTI-KG by users who lack this technical expertise and to decrease workload for those who do not. The VRTI-ChatGPT framework is proposed which uses ChatGPT to interpret requests from users and to facilitate the creation of queries which can be executed on the KG.",no,,,
172,3,sem24-pd,Unveiling SHACL Security Issues: Data Leakage and Validation Bypass,Davan Chiem Dao and Christophe Debruyne,2024-07-04 12:45,1970-01-01 00:00,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/Ikeragnell/shaclExploits
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Knowledge Graphs
SHACL
Security",accept,yes,yes,"This paper aims to uncover security vulnerabilities when using SHACL for data validation in knowledge graphs and enhance security awareness in SHACL and knowledge graph technologies. We employ a knowledge graph developed in a prior study to illustrate potential security issues. While our knowledge graph does not rely on actual data involving users, our study is informed by the well-documented social security domain. This use case is used to identify and illustrate vulnerabilities. The main vulnerability identified is SHACL’s inability to distinguish user input from historical information in the knowledge graph, resulting in two potential exploits: validation bypass and data leakage. This study improves our understanding of security risks in SHACL and knowledge graph technologies, providing valuable insights for practitioners and researchers in this underexplored area in literature.",no,,,
173,3,sem24-pd,AIUP: an ODRL Profile for Expressing AI Use Policies to Support the EU AI Act,"Delaram Golpayegani, Beatriz Esteves, Harshvardhan J. Pandit and Dave Lewis",2024-07-04 13:26,2024-08-13 06:28,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/DelaramGlp/aiup
(Data Availability) 
(Ontology Availability) https://w3id.org/aiup ; ontology documentation
(Demo link) ","AI Act
ODRL
AI use policy
AI risk management
regulatory enforcement
trustworthy AI",accept,yes,yes,"The upcoming EU AI Act requires providers of high-risk AI systems to define and communicate the system's intended purpose -- a key and complex concept upon which many of the Act's obligations rely. To assist with expressing the intended purposes and uses, along with precluded uses as regulated by the AI Act, we extend the Open Digital Rights Language (ODRL) with a profile to express the AI Use Policy (AIUP).   
This open approach to declaring use policies enables explicit and transparent expression of the conditions under which an AI system can be used, benefiting AI application markets beyond the immediate needs of high-risk AI compliance in the EU.  AIUP is available online at https://w3id.org/aiup under the CC-BY-4.0 license.",no,,,
175,3,sem24-pd,Towards SHACL-based Knowledge Graph Transformation of Visual Domain Knowledge,"Simon Steyskal, Stefan Bischof, Josiane Xavier Parreira, Erwin Filtz, Michael Baumgart, David Gruber, Maximilian Liebetreu, Florian Rötzer and Stephan Strommer",2024-07-05 04:09,1970-01-01 00:00,"(Student) 
(Publication Resources) N/A
(Code Availability) 
(Data Availability) 
(Ontology Availability) 
(Demo link) ","Knowledge Graphs
Infinity Maps
Visual Domain Knowledge
Graph Transformation
RDF
SHACL
SHACL Rules
Semantic Web",accept,yes,yes," Effective knowledge representation plays a pivotal role in harnessing the full potential of domain-specific information. Through tools like Infinity Maps, domain knowledge can be easily captured in a visual manner. However, translating these visually intuitive representations to formal, machine-processable formats often necessitates expert knowledge, thereby creating a significant barrier between domain experts and knowledge engineers. While domain experts possess deep understanding of their respective domains, they often lack the formalisation skills required to transform this knowledge into machine-readable formats. Conversely, knowledge engineers can design and implement sophisticated knowledge graphs, but may not have access to the domain-specific expertise necessary for effective knowledge representation.
  To address this challenge, we propose a novel approach that leverages SHACL (Shape Constraint Language) rules to transform visual domain knowledge expressed as Infinity Maps into knowledge graphs. Our method enables domain experts to define their knowledge structures using familiar Infinity Map representations, which are then transformed into standardised knowledge graphs compliant with the SHACL standard.",no,,,
176,3,sem24-pd,SAKE: A Semantic Authoring and Annotation Tool for Knowledge Extraction,"Jan Grau, Kimberly Garcia and Simon Mayer",2024-07-05 11:54,2024-08-15 08:33,"(Student) Yes
(Publication Resources) Yes
(Code Availability) https://github.com/jangrau13/semantics2024_sake
https://github.com/jangrau13/pdf.js
(Data Availability) 
(Ontology Availability) 
(Demo link) https://www.youtube.com/watch?v=6Jndj_rsfMU","Semantic Authoring Tool
Expert Knowledge Extraction
PDF Annotator
RDFa
Knowledge Graph
Semantic Web Tool",accept,yes,yes,"Greenhouse Gas (GHG) accounting is traditionally a lengthy and manual process that requires the expertise of experienced environmental scientists; due to the recognition of the climate crisis through upcoming regulations on GHG accounting around the planet, the demand for tools that can support these environmental experts and accelerate their work is growing considerably at the moment. GHG accounting is merely one application of automated support tools that require the preservation of expert knowledge in a machine-readable and machine-understandable format; across fields, this is highly relevant for automating processes that today can only be performed by individuals with specialized training. In this paper, we present SAKE, a Semantic Authoring and Annotation tool for Knowledge Extraction that allows domain experts with no proficiency in semantic technologies annotating domain-specific PDF files, creating a Knowledge Graph with instances of standardized (or new) ontologies. The resulting Knowledge Graph can then be integrated into systems to automate specialized processes. SAKE has been developed together with domain experts in the field of environmental science and is currently used in the scope of a joint project on GHG accounting.",no,,,
177,3,sem24-pd,AutoGenHR: Automated Generation of Health Reports for Patients at Home,Nicole Merkle,2024-07-05 14:21,2024-07-05 16:45,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/nmerkle/holomatik-ai
https://github.com/nmerkle/nmerkle.github.io/blob/master/HoloBot.html
(Data Availability) 
(Ontology Availability) https://huggingface.co/nmerkle ; Meta-Llama-3-8B-Instruct-ggml-model-Q4_K_M.gguf
https://github.com/nmerkle/holomatik-ai/blob/main/Holomatik-Ontology.ttl
(Demo link) https://nmerkle.github.io/HoloBot.html","Virtual Agents
Knowledge Graphs
Large Language Models
Transformer Neural Networks
Wearables
Digital Twins",accept,yes,yes,"European countries have the highest proportion of people over the age of 50. This implies that age-related diseases, such as cardiovascular diseases, diabetes, cholesterol and dementia, are becoming increasingly common in our aging societies. Additionally, it can be observed that health systems are overburdened due to a lack of medical specialists, resources and capacities. To get an appointment with a medical specialist, patients have to wait up to several months. These circumstances imply that necessary health checks and detection of health risks cannot be carried out in a sufficient manner. Furthermore, the personalisation of medical treatments can hardly be guaranteed. To tackle these problems and enable preventive measures and interventions, a round-the-clock health monitoring, prediction of health risks, and personalised treatment would be necessary. This paper presents Auto Generative Health Reports (AutoGenHR), a work in progress that allows the automated generation of health reports based on acquired context data from round-the-clock monitoring of patients in their domestic environment. In the proposed approach health reports are generated by means of dynamically created knowledge graphs representing patients and context information, and a language model fine-tuned for generating health reports. In this way an early detection of health risks and prevention of avoidable deaths is to be achieved, while patients can stay in their domestic environments without overstretching the capacities of the health system.",no,,,
180,3,sem24-pd,NFDI4DSO: Towards a BFO Compliant Ontology for Data Science,"Genet Asefa Gesese, Jörg Waitelonis, Zongxiong Chen, Sonja Schimmler and Harald Sack",2024-07-05 16:20,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/ISE-FIZKarlsruhe/NFDI4DS-Ontology/tree/main
NFDI4DSO Github Code repository: https://github.com/ISE-FIZKarlsruhe/NFDI4DS-Ontology/tree/develop-1.0.1
(Data Availability) NFDI4DS-KG live: https://nfdi.fiz-karlsruhe.de/4ds/sparql

 
NFDI4DS-KG live: https://nfdi.fiz-karlsruhe.de/4ds/shmarql
(Ontology Availability) NFDI4DSO Ontology Documentaton: https://ise-fizkarlsruhe.github.io/NFDI4DS-Ontology/
(Demo link) ","Data Science
Artificial Intelligence
Ontology
Knowledge Graph
NFDI4DS",accept,yes,yes,"The NFDI4DataScience (NFDI4DS) project aims to enhance the accessibility and interoperability of research data within Data Science (DS) and Artificial Intelligence (AI) by connecting digital artifacts and ensuring they adhere to FAIR (Findable, Accessible, Interoperable, and Reusable) principles. To this end, this poster introduces the NFDI4DS Ontology, which describes resources in DS and AI and models the structure of the NFDI4DS consortium.   Built upon the NFDICore ontology and mapped to the Basic Formal Ontology (BFO), this ontology serves as the foundation for the NFDI4DS knowledge graph currently under development.",no,,,
181,3,sem24-pd,GECKO: A Question Answering System for Official Statistics,"Lucas Lageweg, Jonas Kouwenhoven and Benno Kruit",2024-07-05 19:20,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/lagewel001/GECKO
https://github.com/Jonaskouwenhoven/Thesis-Enhancing-Graph-QA
(Data Availability) 
(Ontology Availability) 
(Demo link) https://gecko.cbs.nl","Knowledge Graph Question Answering
Constrained query generation
Official statistics",accept,yes,yes,"This paper presents GECKO, a knowledge graph-based statistical question answering system currently in beta deployment. GECKO aims to facilitate the retrieval of single statistical values from an extensive database containing over a billion values across more than 4,000 tables. The system integrates a comprehensive framework including data augmentation, entity retrieval, and large language model(LLM)-based query generation. A key feature of the beta deployment is the collection of user feedback, which is critical for improving system performance and accuracy. This feedback mechanism allows users to report issues directly, ensuring continuous improvement based on real-world use.",no,,,
182,3,sem24-pd,Mapping Workbench: A collaborative platform for mapping complex XML data to RDF,"Eugeniu Costetchi, Jana Ahmad, Csongor I. Nyulas, Rashif Rahman and Dumitru Prijilevschi",2024-07-05 21:44,2024-08-12 12:01,"(Student) 
(Publication Resources) Yes
(Code Availability) 
(Data Availability) 
(Ontology Availability) https://meaningfy.ws/mapping-workbench/
(Demo link) https://mw.staging.meaningfy.ws/","XML
RML
RDF
OWL
Data Mapping
Data Transformation
Knowledge Graph Construction",accept,yes,yes,"Mapping Workbench (MWB) facilitates the transformation of XML data into RDF using the RML mapping language, based on a sound test-driven methodology. This paper presents MWB as a comprehensive solution for Semantic Engineers and Data Modellers, offering efficient data mapping, management and validation against ontologies and data shapes. We discuss MWB's key features, the mapping process, and its potential impact on the fields of knowledge graph construction, semantic data interoperability and data integration in general.",no,,,
184,3,sem24-pd,Towards Visual Federated SPARQL Queries,"Kārlis Čerāns, Uldis Bojars, Julija Ovcinnikova, Lelde Lace, Mikus Grasmanis and Arturs Sprogis",2024-07-06 09:28,2024-07-06 12:26,"(Student) 
(Publication Resources) Yes
(Code Availability) ViziQuer, MIT, https://github.com/LUMII-Syslab/viziquer
Data Shape Server, MIT, https://github.com/LUMII-Syslab/data-shape-server
(Data Availability) https://github.com/LUMII-Syslab/viziquer/blob/development/doc/demo/fed_queries/star_wars_example.json ; Example StarWars Project
https://viziquer.lumii.lv/dss/resources/vq_playground.pgsql.gz : ViziQuer playground schema database
(Ontology Availability) https://github.com/LUMII-Syslab/viziquer/tree/development/doc/demo/fed_queries : Supporting material
(Demo link) https://viziquer.app, instructions: https://github.com/LUMII-Syslab/viziquer/tree/development/doc/demo/fed_queries","Data schema
Schema visualization
SPARQL
Federated visual queries",accept,yes,yes,"We demonstrate a method for visual creation of schema-backed federated queries that involves options for schema summary visualizations and context-aware auto-completion of queries, based on schemas of multiple data sets. The method is implemented in the ViziQuer tool context, based on the collection of multiple stored data schemas, including ones for DBPedia and Wikidata. The environment for schema visualization and visual federated query creation is available online as a playground and as a local open-source installation.",no,,,
187,3,sem24-pd,Post-Hoc Insights: Natural-Language Explanations for AI-Enhanced/-Integrated Software Systems,"Dennis Schiese, Aleksandr Perevalov and Andreas Both",2024-07-06 11:31,1970-01-01 00:00,"(Student) 
(Publication Resources) Yes
(Code Availability) https://github.com/WSE-research/qanary-explainability-frontend
https://github.com/WSE-research/qanary-explanation-service
https://hub.docker.com/r/wseresearch/qanary-explainability-frontend
(Data Availability) 
(Ontology Availability) 
(Demo link) http://demos.swe.htwk-leipzig.de:40119/","Explainability
Component-based Systems
Natural-Language Generation
Large Language Models",accept,yes,yes,"In this paper, we aim to address the problem of explainability as a broad research area for the specific case of component-based systems. As we will tackle this problem from the data perspective, we'll utilize two different types of data, namely SPARQL and RDF triples. As a goal of this data-driven explainability, we want to provide a greater and deeper understanding of the system and its process. To achieve this, we present a demonstrator to create explanations for component-based Question Answering systems. There, the explanations are created with the use of different generative AI systems and for comparison based on templates. We publish this as an open-sourced web application online, which is freely available to all users.",no,,,
